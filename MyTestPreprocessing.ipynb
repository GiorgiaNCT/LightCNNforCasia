{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MyTestPreprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "n88iRlHnIqsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install imgaug==0.2.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VXHLXA1Ivcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install Pillow==7.0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5pyDKvGIxNp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "!pip install -q -U opencv-python\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.client import device_lib\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDTK2f2PQJqn",
        "colab_type": "code",
        "outputId": "2863d295-6396-450a-aeda-c3faea14d354",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "!git clone https://github.com/AlfredXiangWu/LightCNN.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'LightCNN'...\n",
            "remote: Enumerating objects: 64, done.\u001b[K\n",
            "remote: Total 64 (delta 0), reused 0 (delta 0), pack-reused 64\u001b[K\n",
            "Unpacking objects: 100% (64/64), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BH3KctHRQNYW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(\"LightCNN\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js6zLP9JkDi9",
        "colab_type": "text"
      },
      "source": [
        "DataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5Q4vx8yrq4V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "9ebbbe68-3fc4-49f8-bc5b-f848289f8658"
      },
      "source": [
        "!git clone https://github.com/Mendru/facenet.git"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'facenet'...\n",
            "remote: Enumerating objects: 3084, done.\u001b[K\n",
            "remote: Total 3084 (delta 0), reused 0 (delta 0), pack-reused 3084\u001b[K\n",
            "Receiving objects: 100% (3084/3084), 164.01 MiB | 36.35 MiB/s, done.\n",
            "Resolving deltas: 100% (17/17), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nLr1hcO9AuW",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dwc-PPVNC8-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install face-recognition"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5z5tY-9bDAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/LightCNN/facenet/CASIA-FaceV5-160/* /content/LightCNN/CASIA3/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGwe8E3zCjz6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import face_recognition"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfvERv53WH7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing_first_cut():\n",
        "    #print('ciao')\n",
        "    directory = 'CASIA3'\n",
        "    for filedir in os.listdir(directory):\n",
        "        #print(directory + '/' + filedir)\n",
        "        for filename in os.listdir(directory+'/'+filedir):\n",
        "          #print(directory+'/'+filedir+'/'+filename)\n",
        "          if filename[0] != '.':\n",
        "            img_path = str(directory+'/'+filedir+'/'+filename)\n",
        "            image = face_recognition.load_image_file(img_path)\n",
        "            face_locations = face_recognition.face_locations(image)\n",
        "            #print(\"****\")\n",
        "            #print(directory+'/'+filedir+'/'+filename)\n",
        "            #print(face_locations)\n",
        "            #print(image.shape)\n",
        "            #print(f'There are {len(face_locations)} people in this image')\n",
        "\n",
        "            if(len(face_locations) > 0):\n",
        "              face_location = face_locations[0]\n",
        "              top, right, bottom, left = face_location\n",
        "              if bottom - top != 128:\n",
        "                padding_vertical = 128 - (bottom - top)\n",
        "              else:\n",
        "                padding_vertical = 0\n",
        "  \n",
        "              if right - left != 128:\n",
        "                padding_horizontal = 128 - ( right - left)\n",
        "              else:\n",
        "                padding_horizontal = 0\n",
        "              \n",
        "              marg_left = left - int(padding_horizontal/2)\n",
        "              if marg_left < 0:\n",
        "                marg_left = 0\n",
        "              \n",
        "              marg_top = top - int(padding_vertical/2)\n",
        "              if marg_top < 0:\n",
        "                marg_top = 0\n",
        "              #print(\"l: \", left - int(padding_horizontal/2), \" r:\", right + int(padding_horizontal/2), \" b: \", bottom + int(padding_vertical/2), \" t: \", top - int(padding_vertical/2))\n",
        "              face_image = image[marg_top :bottom + int(padding_vertical/2), marg_left:right + int(padding_horizontal/2)]\n",
        "              pil_image = Image.fromarray(face_image)\n",
        "              pil_image.save(img_path)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-qm49WgWJ_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessing_first_cut()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFyj5POlUd0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing():\n",
        "    #print('ciao')\n",
        "    directory = 'CASIA3'\n",
        "    for filedir in os.listdir(directory):\n",
        "        #print(directory + '/' + filedir)\n",
        "        for filename in os.listdir(directory+'/'+filedir):\n",
        "          #print(directory+'/'+filedir+'/'+filename)\n",
        "          if filename[0] != '.':\n",
        "            img_path = str(directory+'/'+filedir+'/'+filename)\n",
        "            image = cv2.imread(img_path)\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            image = cv2.resize(image, (128, 128), interpolation=cv2.INTER_CUBIC)\n",
        "            norm_img = np.zeros((300, 300))\n",
        "            image = cv2.normalize(image, norm_img, 0, 255, cv2.NORM_MINMAX)\n",
        "\n",
        "            cv2.imwrite(img_path, image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7701PAcEUwTr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessing()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swNax91nsEfs",
        "colab_type": "code",
        "outputId": "afc508fc-d794-4c8b-d252-c540ceed634f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "source": [
        "image = cv2.imread(\"CASIA3/202/202_3.png\")\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "print(image.shape)\n",
        "plt.imshow(image, cmap=\"gray\")\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128, 128)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2zM387ciknw",
        "colab_type": "text"
      },
      "source": [
        "Split Train, Test and Val"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT6VqGzEtm_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQL9tbZDsAwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pp_save_clean_dataset():\n",
        "    #print('ciao')\n",
        "    directory = 'CASIA3'\n",
        "    ftrain = open('train.txt','w+')\n",
        "    fval = open('val.txt',\"w+\")\n",
        "    ftest = open('test.txt','w+')\n",
        "    jclass=0\n",
        "    for filedir in os.listdir(directory):\n",
        "        #print(directory + '/' + filedir)\n",
        "        jclass+=1\n",
        "        i = 0\n",
        "        for filename in os.listdir(directory+'/'+filedir):\n",
        "          #print(directory+'/'+filedir+'/'+filename)\n",
        "          if filename[0] != '.':\n",
        "            rnd = random.random()\n",
        "            #print(rnd)\n",
        "            # 70% train set, 20% val set, 10% test set.\n",
        "            if rnd<.7:\n",
        "              #print(\"rand7\")\n",
        "              ftrain.write( filedir+'/'+filename+' '+str(jclass)+'\\n')\n",
        "            elif rnd<.9:\n",
        "              #print(\"rand9\")\n",
        "              fval.write( filedir+'/'+filename+' '+str(jclass)+'\\n')\n",
        "            else:\n",
        "              #print(\"else\")\n",
        "              ftest.write( filedir+'/'+filename+' '+str(jclass)+'\\n')\n",
        "              i+=1\n",
        "    ftrain.close()\n",
        "    fval.close()\n",
        "    ftest.close()\n",
        "    print(jclass)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9w56xcGtj3t",
        "colab_type": "code",
        "outputId": "ca66decf-01cb-4224-82d3-945079ea28af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pp_save_clean_dataset()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twO9bG2Pct0w",
        "colab_type": "text"
      },
      "source": [
        "START TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuSguSUXc49x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from light_cnn import LightCNN_9Layers, LightCNN_29Layers, LightCNN_29Layers_v2\n",
        "from load_imglist import ImageList"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riNCn2_Ycr7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "        ImageList(root=\"CASIA3/\", fileList=\"train.txt\", \n",
        "            transform=transforms.Compose([ \n",
        "                transforms.RandomCrop(128),\n",
        "                transforms.RandomHorizontalFlip(), \n",
        "                transforms.ToTensor(),\n",
        "            ])),\n",
        "        batch_size=32, shuffle=True,\n",
        "        num_workers=16, pin_memory=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPbjmKZCc2jw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_loader = torch.utils.data.DataLoader(\n",
        "        ImageList(root=\"CASIA3/\", fileList=\"val.txt\", \n",
        "            transform=transforms.Compose([ \n",
        "                transforms.CenterCrop(128),\n",
        "                transforms.ToTensor(),\n",
        "            ])),\n",
        "        batch_size=32, shuffle=False,\n",
        "        num_workers=16, pin_memory=True)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jew-nRVhc85s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LightCNN_29Layers(num_classes=501)\n",
        "model = torch.nn.DataParallel(model).cuda()\n",
        "# large lr for last fc parameters\n",
        "params = []\n",
        "for name, value in model.named_parameters():\n",
        "  if 'bias' in name:\n",
        "    if 'fc2' in name:\n",
        "      params += [{'params':value, 'lr': 20 * 0.01, 'weight_decay': 0}]\n",
        "    else:\n",
        "      params += [{'params':value, 'lr': 2 * 0.01, 'weight_decay': 0}]\n",
        "  else:\n",
        "    if 'fc2' in name:\n",
        "      params += [{'params':value, 'lr': 10 * 0.01}]\n",
        "    else:\n",
        "      params += [{'params':value, 'lr': 1 * 0.01}]\n",
        "\n",
        "weight_decay=1e-4\n",
        "optimizer = torch.optim.SGD(params, 0.01,\n",
        "                                momentum=0.9,\n",
        "                                weight_decay=weight_decay )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TqNtyPqdU3r",
        "colab_type": "text"
      },
      "source": [
        "Set resume to True, if there is an older model to upload"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4SVQEq_dRY2",
        "colab_type": "code",
        "outputId": "1c695b71-5756-4358-d6c2-d2d30c5ca30f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "    resume = True\n",
        "    # optionally resume from a checkpoint\n",
        "    if resume:\n",
        "        if os.path.isfile(\"saveFolder2/lightCNN_151_checkpoint.pth.tar\"):\n",
        "            print(\"=> loading checkpoint '{}'\".format(\"saveFolder2/lightCNN_151_checkpoint.pth.tar\"))\n",
        "            checkpoint = torch.load(\"saveFolder2/lightCNN_151_checkpoint.pth.tar\")\n",
        "            start_epoch = checkpoint['epoch']\n",
        "            model.load_state_dict(checkpoint['state_dict'])\n",
        "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
        "                  .format(\"saveFolder2/lightCNN_151_checkpoint.pth.tar\", checkpoint['epoch']))\n",
        "        else:\n",
        "            print(\"=> no checkpoint found at '{}'\".format(\"saveFolder2/lightCNN_151_checkpoint.pth.tar\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=> loading checkpoint 'saveFolder2/lightCNN_151_checkpoint.pth.tar'\n",
            "=> loaded checkpoint 'saveFolder2/lightCNN_151_checkpoint.pth.tar' (epoch 151)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bO_IMQLpdoPD",
        "colab_type": "code",
        "outputId": "890b7588-9929-4f94-a26a-58f29bd8aab7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cudnn.benchmark = True\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion.cuda()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CrossEntropyLoss()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0qdyGj9dtkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val   = 0\n",
        "        self.avg   = 0\n",
        "        self.sum   = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val   = val\n",
        "        self.sum   += val * n\n",
        "        self.count += n\n",
        "        self.avg   = self.sum / self.count\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    scale = 0.457305051927326\n",
        "    step  = 10\n",
        "    lr = 0.01 * (scale ** (epoch // step))\n",
        "    print('lr: {}'.format(lr))\n",
        "    if (epoch != 0) & (epoch % step == 0):\n",
        "        print('Change lr')\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = param_group['lr'] * scale\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred    = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AuJ2nQWdwVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(val_loader, model, criterion):\n",
        "    batch_time = AverageMeter()\n",
        "    losses     = AverageMeter()\n",
        "    top1       = AverageMeter()\n",
        "    top5       = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(val_loader):\n",
        "        input      = input.cuda()\n",
        "        target     = target.cuda()\n",
        "        input_var  = torch.autograd.Variable(input, volatile=True)\n",
        "        target_var = torch.autograd.Variable(target, volatile=True)\n",
        "\n",
        "        # compute output\n",
        "        output, _ = model(input_var)\n",
        "        loss   = criterion(output, target_var)\n",
        "        # measure accuracy and record loss\n",
        "        prec1, prec5 = accuracy(output.data, target, topk=(1,5))\n",
        "        loss_data = loss.data\n",
        "        losses.update(loss.data.item(), input.size(0))\n",
        "        top1.update(prec1.item(), input.size(0))\n",
        "        top5.update(prec5.item(), input.size(0))\n",
        "\n",
        "\n",
        "\n",
        "    print('\\nTest set: Average loss: {}, Accuracy: ({})\\n'.format(losses.avg, top1.avg))\n",
        "\n",
        "    return top1.avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcwXjZmrdzvj",
        "colab_type": "code",
        "outputId": "8c0108b2-8cb9-4f55-c3c2-e7aeddd9785e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "validate(val_loader, model, criterion)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 6.2067608447210025, Accuracy: (0.20242914979757085)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.20242914979757085"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeF18IYo4_94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_list = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD0jQRY3d1_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time  = AverageMeter()\n",
        "    losses     = AverageMeter()\n",
        "    top1       = AverageMeter()\n",
        "    top5       = AverageMeter()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        input      = input.cuda()\n",
        "        target     = target.cuda()\n",
        "        input_var  = torch.autograd.Variable(input)\n",
        "        target_var = torch.autograd.Variable(target)\n",
        "\n",
        "        # compute output\n",
        "        output, _ = model(input_var)\n",
        "        loss   = criterion(output, target_var)\n",
        "        # measure accuracy and record loss\n",
        "        prec1, prec5 = accuracy(output.data, target, topk=(1,5))\n",
        "        loss_data = loss.data\n",
        "        losses.update(loss_data.item(), input.size(0))\n",
        "        top1.update(prec1.item(), input.size(0))\n",
        "        top5.update(prec5.item(), input.size(0))\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        # Save variables\n",
        "        loss_list.append([losses.avg, top1.avg])\n",
        "\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
        "                  'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
        "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
        "                   data_time=data_time, loss=losses, top1=top1, top5=top5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfdH4jrld5Cd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_checkpoint(state, filename):\n",
        "    torch.save(state, filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7oE_Hz6d7vo",
        "colab_type": "code",
        "outputId": "c46ace15-fa94-4fd8-d660-3803bd2adb1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(0, 502):\n",
        "  adjust_learning_rate(optimizer, epoch)\n",
        "\n",
        "  # train for one epoch\n",
        "  train(train_loader, model, criterion, optimizer, epoch)\n",
        "\n",
        "  # evaluate on validation set\n",
        "  prec1 = validate(val_loader, model, criterion)\n",
        "\n",
        "  if(epoch % 50 == 0):\n",
        "    save_name = \"saveFolder2/\" + 'lightCNN_' + str(epoch+1) + '_checkpoint.pth.tar'\n",
        "    save_checkpoint({\n",
        "        'epoch': epoch + 1,\n",
        "        'arch': 'LightCNN',\n",
        "        'state_dict': model.state_dict(),\n",
        "        'prec1': prec1,\n",
        "        }, save_name)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lr: 0.01\n",
            "Epoch: [0][0/55]\tTime 1.189 (1.189)\tData 0.580 (0.580)\tLoss 6.2336 (6.2336)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 6.241929355420564, Accuracy: (0.0)\n",
            "\n",
            "lr: 0.01\n",
            "Epoch: [1][0/55]\tTime 0.764 (0.764)\tData 0.552 (0.552)\tLoss 6.2230 (6.2230)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
            "\n",
            "Test set: Average loss: 6.282335076737501, Accuracy: (0.0)\n",
            "\n",
            "lr: 0.01\n",
            "Epoch: [2][0/55]\tTime 0.736 (0.736)\tData 0.578 (0.578)\tLoss 6.1992 (6.1992)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
            "\n",
            "Test set: Average loss: 6.306607943314773, Accuracy: (0.0)\n",
            "\n",
            "lr: 0.01\n",
            "Epoch: [3][0/55]\tTime 0.762 (0.762)\tData 0.546 (0.546)\tLoss 6.1976 (6.1976)\tPrec@1 0.000 (0.000)\tPrec@5 3.125 (3.125)\n",
            "\n",
            "Test set: Average loss: 6.328931671405128, Accuracy: (0.0)\n",
            "\n",
            "lr: 0.01\n",
            "Epoch: [4][0/55]\tTime 0.713 (0.713)\tData 0.548 (0.548)\tLoss 6.2003 (6.2003)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
            "\n",
            "Test set: Average loss: 6.373574463462058, Accuracy: (0.0)\n",
            "\n",
            "lr: 0.01\n",
            "Epoch: [5][0/55]\tTime 0.726 (0.726)\tData 0.544 (0.544)\tLoss 6.1940 (6.1940)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
            "\n",
            "Test set: Average loss: 6.371350352098102, Accuracy: (0.0)\n",
            "\n",
            "lr: 0.01\n",
            "Epoch: [6][0/55]\tTime 0.701 (0.701)\tData 0.545 (0.545)\tLoss 6.1540 (6.1540)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
            "\n",
            "Test set: Average loss: 6.377386095070163, Accuracy: (0.0)\n",
            "\n",
            "lr: 0.01\n",
            "Epoch: [7][0/55]\tTime 0.712 (0.712)\tData 0.540 (0.540)\tLoss 6.2249 (6.2249)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
            "\n",
            "Test set: Average loss: 6.408726601465511, Accuracy: (0.0)\n",
            "\n",
            "lr: 0.01\n",
            "Epoch: [8][0/55]\tTime 0.773 (0.773)\tData 0.573 (0.573)\tLoss 6.2335 (6.2335)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
            "\n",
            "Test set: Average loss: 6.397028847744591, Accuracy: (0.0)\n",
            "\n",
            "lr: 0.01\n",
            "Epoch: [9][0/55]\tTime 0.742 (0.742)\tData 0.528 (0.528)\tLoss 6.2128 (6.2128)\tPrec@1 0.000 (0.000)\tPrec@5 3.125 (3.125)\n",
            "\n",
            "Test set: Average loss: 6.4314262644964675, Accuracy: (0.0)\n",
            "\n",
            "lr: 0.00457305051927326\n",
            "Change lr\n",
            "Epoch: [10][0/55]\tTime 0.719 (0.719)\tData 0.553 (0.553)\tLoss 6.1872 (6.1872)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
            "\n",
            "Test set: Average loss: 6.416224288554327, Accuracy: (0.0)\n",
            "\n",
            "lr: 0.00457305051927326\n",
            "Epoch: [11][0/55]\tTime 0.728 (0.728)\tData 0.557 (0.557)\tLoss 6.1396 (6.1396)\tPrec@1 3.125 (3.125)\tPrec@5 3.125 (3.125)\n",
            "\n",
            "Test set: Average loss: 6.421385697507665, Accuracy: (0.0)\n",
            "\n",
            "lr: 0.00457305051927326\n",
            "Epoch: [12][0/55]\tTime 0.763 (0.763)\tData 0.562 (0.562)\tLoss 6.1617 (6.1617)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
            "\n",
            "Test set: Average loss: 6.42506844022496, Accuracy: (0.0)\n",
            "\n",
            "lr: 0.00457305051927326\n",
            "Epoch: [13][0/55]\tTime 0.766 (0.766)\tData 0.615 (0.615)\tLoss 6.1799 (6.1799)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
            "\n",
            "Test set: Average loss: 6.4265201873624855, Accuracy: (0.0)\n",
            "\n",
            "lr: 0.00457305051927326\n",
            "Epoch: [14][0/55]\tTime 0.743 (0.743)\tData 0.535 (0.535)\tLoss 6.1999 (6.1999)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
            "\n",
            "Test set: Average loss: 6.431010402648555, Accuracy: (0.0)\n",
            "\n",
            "lr: 0.00457305051927326\n",
            "Epoch: [15][0/55]\tTime 0.742 (0.742)\tData 0.552 (0.552)\tLoss 6.1666 (6.1666)\tPrec@1 6.250 (6.250)\tPrec@5 9.375 (9.375)\n",
            "\n",
            "Test set: Average loss: 6.433666308399154, Accuracy: (0.0)\n",
            "\n",
            "lr: 0.00457305051927326\n",
            "Epoch: [16][0/55]\tTime 0.784 (0.784)\tData 0.591 (0.591)\tLoss 6.1774 (6.1774)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
            "\n",
            "Test set: Average loss: 6.433299383171174, Accuracy: (0.0)\n",
            "\n",
            "lr: 0.00457305051927326\n",
            "Epoch: [17][0/55]\tTime 0.803 (0.803)\tData 0.569 (0.569)\tLoss 6.1904 (6.1904)\tPrec@1 0.000 (0.000)\tPrec@5 6.250 (6.250)\n",
            "\n",
            "Test set: Average loss: 6.438643538517508, Accuracy: (0.0)\n",
            "\n",
            "lr: 0.00457305051927326\n",
            "Epoch: [18][0/55]\tTime 0.777 (0.777)\tData 0.559 (0.559)\tLoss 6.1823 (6.1823)\tPrec@1 3.125 (3.125)\tPrec@5 6.250 (6.250)\n",
            "\n",
            "Test set: Average loss: 6.439304351806641, Accuracy: (0.0)\n",
            "\n",
            "lr: 0.00457305051927326\n",
            "Epoch: [19][0/55]\tTime 0.729 (0.729)\tData 0.560 (0.560)\tLoss 6.1284 (6.1284)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
            "\n",
            "Test set: Average loss: 6.441467095965798, Accuracy: (0.0)\n",
            "\n",
            "lr: 0.0020912791051825434\n",
            "Change lr\n",
            "Epoch: [20][0/55]\tTime 0.770 (0.770)\tData 0.525 (0.525)\tLoss 6.1061 (6.1061)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
            "\n",
            "Test set: Average loss: 6.4428345471741215, Accuracy: (0.0)\n",
            "\n",
            "lr: 0.0020912791051825434\n",
            "Epoch: [21][0/55]\tTime 0.757 (0.757)\tData 0.532 (0.532)\tLoss 6.2002 (6.2002)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
            "\n",
            "Test set: Average loss: 6.44342291017293, Accuracy: (0.0)\n",
            "\n",
            "lr: 0.0020912791051825434\n",
            "Epoch: [22][0/55]\tTime 0.731 (0.731)\tData 0.544 (0.544)\tLoss 6.1643 (6.1643)\tPrec@1 3.125 (3.125)\tPrec@5 6.250 (6.250)\n",
            "\n",
            "Test set: Average loss: 6.444687293126033, Accuracy: (0.0)\n",
            "\n",
            "lr: 0.0020912791051825434\n",
            "Epoch: [23][0/55]\tTime 0.710 (0.710)\tData 0.557 (0.557)\tLoss 6.2064 (6.2064)\tPrec@1 0.000 (0.000)\tPrec@5 6.250 (6.250)\n",
            "\n",
            "Test set: Average loss: 6.443911658607513, Accuracy: (0.0)\n",
            "\n",
            "lr: 0.0020912791051825434\n",
            "Epoch: [24][0/55]\tTime 0.730 (0.730)\tData 0.550 (0.550)\tLoss 6.1243 (6.1243)\tPrec@1 0.000 (0.000)\tPrec@5 3.125 (3.125)\n",
            "\n",
            "Test set: Average loss: 6.4434983122204, Accuracy: (0.0)\n",
            "\n",
            "lr: 0.0020912791051825434\n",
            "Epoch: [25][0/55]\tTime 0.734 (0.734)\tData 0.558 (0.558)\tLoss 6.1273 (6.1273)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
            "\n",
            "Test set: Average loss: 6.37927250920037, Accuracy: (0.0)\n",
            "\n",
            "lr: 0.0020912791051825434\n",
            "Epoch: [26][0/55]\tTime 0.780 (0.780)\tData 0.582 (0.582)\tLoss 6.0455 (6.0455)\tPrec@1 0.000 (0.000)\tPrec@5 3.125 (3.125)\n",
            "\n",
            "Test set: Average loss: 6.20031973031851, Accuracy: (0.20242914979757085)\n",
            "\n",
            "lr: 0.0020912791051825434\n",
            "Epoch: [27][0/55]\tTime 0.732 (0.732)\tData 0.531 (0.531)\tLoss 5.5950 (5.5950)\tPrec@1 0.000 (0.000)\tPrec@5 9.375 (9.375)\n",
            "\n",
            "Test set: Average loss: 6.348529919921628, Accuracy: (0.0)\n",
            "\n",
            "lr: 0.0020912791051825434\n",
            "Epoch: [28][0/55]\tTime 0.777 (0.777)\tData 0.579 (0.579)\tLoss 6.0087 (6.0087)\tPrec@1 0.000 (0.000)\tPrec@5 3.125 (3.125)\n",
            "\n",
            "Test set: Average loss: 6.087369897587579, Accuracy: (0.6072874493927125)\n",
            "\n",
            "lr: 0.0020912791051825434\n",
            "Epoch: [29][0/55]\tTime 0.736 (0.736)\tData 0.550 (0.550)\tLoss 5.3931 (5.3931)\tPrec@1 0.000 (0.000)\tPrec@5 6.250 (6.250)\n",
            "\n",
            "Test set: Average loss: 6.021780519832967, Accuracy: (1.214574898785425)\n",
            "\n",
            "lr: 0.0009563524997900349\n",
            "Change lr\n",
            "Epoch: [30][0/55]\tTime 0.728 (0.728)\tData 0.548 (0.548)\tLoss 5.3678 (5.3678)\tPrec@1 0.000 (0.000)\tPrec@5 6.250 (6.250)\n",
            "\n",
            "Test set: Average loss: 5.8833275323937295, Accuracy: (2.2267206477732793)\n",
            "\n",
            "lr: 0.0009563524997900349\n",
            "Epoch: [31][0/55]\tTime 0.806 (0.806)\tData 0.558 (0.558)\tLoss 5.3035 (5.3035)\tPrec@1 6.250 (6.250)\tPrec@5 21.875 (21.875)\n",
            "\n",
            "Test set: Average loss: 5.8617241585302935, Accuracy: (0.8097165972597686)\n",
            "\n",
            "lr: 0.0009563524997900349\n",
            "Epoch: [32][0/55]\tTime 0.799 (0.799)\tData 0.572 (0.572)\tLoss 5.2292 (5.2292)\tPrec@1 6.250 (6.250)\tPrec@5 9.375 (9.375)\n",
            "\n",
            "Test set: Average loss: 5.8131779813573425, Accuracy: (2.834008095235477)\n",
            "\n",
            "lr: 0.0009563524997900349\n",
            "Epoch: [33][0/55]\tTime 0.766 (0.766)\tData 0.591 (0.591)\tLoss 4.9192 (4.9192)\tPrec@1 6.250 (6.250)\tPrec@5 18.750 (18.750)\n",
            "\n",
            "Test set: Average loss: 5.763989608780093, Accuracy: (2.42914979757085)\n",
            "\n",
            "lr: 0.0009563524997900349\n",
            "Epoch: [34][0/55]\tTime 0.771 (0.771)\tData 0.570 (0.570)\tLoss 4.0524 (4.0524)\tPrec@1 9.375 (9.375)\tPrec@5 31.250 (31.250)\n",
            "\n",
            "Test set: Average loss: 5.675412734027816, Accuracy: (3.6437246963562755)\n",
            "\n",
            "lr: 0.0009563524997900349\n",
            "Epoch: [35][0/55]\tTime 0.742 (0.742)\tData 0.550 (0.550)\tLoss 4.2214 (4.2214)\tPrec@1 12.500 (12.500)\tPrec@5 31.250 (31.250)\n",
            "\n",
            "Test set: Average loss: 5.490055877670103, Accuracy: (6.477732793522267)\n",
            "\n",
            "lr: 0.0009563524997900349\n",
            "Epoch: [36][0/55]\tTime 0.789 (0.789)\tData 0.566 (0.566)\tLoss 3.7275 (3.7275)\tPrec@1 21.875 (21.875)\tPrec@5 43.750 (43.750)\n",
            "\n",
            "Test set: Average loss: 5.71294116201671, Accuracy: (5.87044534219904)\n",
            "\n",
            "lr: 0.0009563524997900349\n",
            "Epoch: [37][0/55]\tTime 0.719 (0.719)\tData 0.560 (0.560)\tLoss 3.5695 (3.5695)\tPrec@1 15.625 (15.625)\tPrec@5 43.750 (43.750)\n",
            "\n",
            "Test set: Average loss: 5.418578674918727, Accuracy: (8.704453439365032)\n",
            "\n",
            "lr: 0.0009563524997900349\n",
            "Epoch: [38][0/55]\tTime 0.716 (0.716)\tData 0.566 (0.566)\tLoss 3.0845 (3.0845)\tPrec@1 28.125 (28.125)\tPrec@5 50.000 (50.000)\n",
            "\n",
            "Test set: Average loss: 5.239005806957662, Accuracy: (10.931174085207797)\n",
            "\n",
            "lr: 0.0009563524997900349\n",
            "Epoch: [39][0/55]\tTime 0.756 (0.756)\tData 0.543 (0.543)\tLoss 3.3615 (3.3615)\tPrec@1 28.125 (28.125)\tPrec@5 56.250 (56.250)\n",
            "\n",
            "Test set: Average loss: 5.442297752086933, Accuracy: (9.919028338150456)\n",
            "\n",
            "lr: 0.00043734482957731\n",
            "Change lr\n",
            "Epoch: [40][0/55]\tTime 0.835 (0.835)\tData 0.595 (0.595)\tLoss 2.0056 (2.0056)\tPrec@1 56.250 (56.250)\tPrec@5 84.375 (84.375)\n",
            "\n",
            "Test set: Average loss: 5.450074859958911, Accuracy: (11.94331983419565)\n",
            "\n",
            "lr: 0.00043734482957731\n",
            "Epoch: [41][0/55]\tTime 0.758 (0.758)\tData 0.553 (0.553)\tLoss 2.3696 (2.3696)\tPrec@1 34.375 (34.375)\tPrec@5 68.750 (68.750)\n",
            "\n",
            "Test set: Average loss: 5.575940583881579, Accuracy: (12.348178133790793)\n",
            "\n",
            "lr: 0.00043734482957731\n",
            "Epoch: [42][0/55]\tTime 0.765 (0.765)\tData 0.567 (0.567)\tLoss 2.0098 (2.0098)\tPrec@1 53.125 (53.125)\tPrec@5 78.125 (78.125)\n",
            "\n",
            "Test set: Average loss: 5.485549465364773, Accuracy: (13.765182182373788)\n",
            "\n",
            "lr: 0.00043734482957731\n",
            "Epoch: [43][0/55]\tTime 0.784 (0.784)\tData 0.547 (0.547)\tLoss 2.0226 (2.0226)\tPrec@1 43.750 (43.750)\tPrec@5 81.250 (81.250)\n",
            "\n",
            "Test set: Average loss: 5.533978589633215, Accuracy: (16.19433197994464)\n",
            "\n",
            "lr: 0.00043734482957731\n",
            "Epoch: [44][0/55]\tTime 0.777 (0.777)\tData 0.555 (0.555)\tLoss 1.7329 (1.7329)\tPrec@1 53.125 (53.125)\tPrec@5 84.375 (84.375)\n",
            "\n",
            "Test set: Average loss: 5.717620283968536, Accuracy: (17.00404857913492)\n",
            "\n",
            "lr: 0.00043734482957731\n",
            "Epoch: [45][0/55]\tTime 0.707 (0.707)\tData 0.529 (0.529)\tLoss 1.4184 (1.4184)\tPrec@1 56.250 (56.250)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 5.672828958102083, Accuracy: (18.016194328122776)\n",
            "\n",
            "lr: 0.00043734482957731\n",
            "Epoch: [46][0/55]\tTime 0.757 (0.757)\tData 0.605 (0.605)\tLoss 1.0575 (1.0575)\tPrec@1 68.750 (68.750)\tPrec@5 93.750 (93.750)\n",
            "\n",
            "Test set: Average loss: 5.865376177104379, Accuracy: (17.813765178325205)\n",
            "\n",
            "lr: 0.00043734482957731\n",
            "Epoch: [47][0/55]\tTime 0.737 (0.737)\tData 0.554 (0.554)\tLoss 1.2384 (1.2384)\tPrec@1 71.875 (71.875)\tPrec@5 90.625 (90.625)\n",
            "\n",
            "Test set: Average loss: 5.709000834569275, Accuracy: (18.218623477920346)\n",
            "\n",
            "lr: 0.00043734482957731\n",
            "Epoch: [48][0/55]\tTime 0.744 (0.744)\tData 0.553 (0.553)\tLoss 1.3503 (1.3503)\tPrec@1 68.750 (68.750)\tPrec@5 90.625 (90.625)\n",
            "\n",
            "Test set: Average loss: 5.649924378646047, Accuracy: (20.040485837681572)\n",
            "\n",
            "lr: 0.00043734482957731\n",
            "Epoch: [49][0/55]\tTime 0.739 (0.739)\tData 0.550 (0.550)\tLoss 0.9414 (0.9414)\tPrec@1 68.750 (68.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 5.815122309001351, Accuracy: (20.445344137276717)\n",
            "\n",
            "lr: 0.00019999999999999928\n",
            "Change lr\n",
            "Epoch: [50][0/55]\tTime 0.783 (0.783)\tData 0.550 (0.550)\tLoss 0.6277 (0.6277)\tPrec@1 78.125 (78.125)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 6.3698508420936495, Accuracy: (19.635627538086432)\n",
            "\n",
            "lr: 0.00019999999999999928\n",
            "Epoch: [51][0/55]\tTime 0.727 (0.727)\tData 0.562 (0.562)\tLoss 0.4717 (0.4717)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 6.551945724950628, Accuracy: (21.862348185859712)\n",
            "\n",
            "lr: 0.00019999999999999928\n",
            "Epoch: [52][0/55]\tTime 0.753 (0.753)\tData 0.545 (0.545)\tLoss 1.1792 (1.1792)\tPrec@1 75.000 (75.000)\tPrec@5 93.750 (93.750)\n",
            "\n",
            "Test set: Average loss: 6.81086438677089, Accuracy: (20.647773287074287)\n",
            "\n",
            "lr: 0.00019999999999999928\n",
            "Epoch: [53][0/55]\tTime 0.743 (0.743)\tData 0.561 (0.561)\tLoss 0.8983 (0.8983)\tPrec@1 68.750 (68.750)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 6.781287073606421, Accuracy: (19.43319838828886)\n",
            "\n",
            "lr: 0.00019999999999999928\n",
            "Epoch: [54][0/55]\tTime 0.740 (0.740)\tData 0.568 (0.568)\tLoss 0.3964 (0.3964)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 6.877362343946449, Accuracy: (23.68421053403785)\n",
            "\n",
            "lr: 0.00019999999999999928\n",
            "Epoch: [55][0/55]\tTime 0.695 (0.695)\tData 0.546 (0.546)\tLoss 0.3080 (0.3080)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.206024266447615, Accuracy: (22.267206485454853)\n",
            "\n",
            "lr: 0.00019999999999999928\n",
            "Epoch: [56][0/55]\tTime 0.778 (0.778)\tData 0.582 (0.582)\tLoss 0.4730 (0.4730)\tPrec@1 90.625 (90.625)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 6.814481183102257, Accuracy: (22.874493934847564)\n",
            "\n",
            "lr: 0.00019999999999999928\n",
            "Epoch: [57][0/55]\tTime 0.780 (0.780)\tData 0.556 (0.556)\tLoss 0.1507 (0.1507)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.188802282819863, Accuracy: (23.279352234442708)\n",
            "\n",
            "lr: 0.00019999999999999928\n",
            "Epoch: [58][0/55]\tTime 0.777 (0.777)\tData 0.556 (0.556)\tLoss 0.4058 (0.4058)\tPrec@1 84.375 (84.375)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.158949234225006, Accuracy: (23.076923084645138)\n",
            "\n",
            "lr: 0.00019999999999999928\n",
            "Epoch: [59][0/55]\tTime 0.730 (0.730)\tData 0.585 (0.585)\tLoss 0.1985 (0.1985)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.031398753888212, Accuracy: (22.874493934847564)\n",
            "\n",
            "lr: 9.146101038546488e-05\n",
            "Change lr\n",
            "Epoch: [60][0/55]\tTime 0.750 (0.750)\tData 0.563 (0.563)\tLoss 0.4506 (0.4506)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 6.781879973315034, Accuracy: (23.68421053403785)\n",
            "\n",
            "lr: 9.146101038546488e-05\n",
            "Epoch: [61][0/55]\tTime 0.711 (0.711)\tData 0.558 (0.558)\tLoss 0.3160 (0.3160)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.3634875332295655, Accuracy: (24.08906883363299)\n",
            "\n",
            "lr: 9.146101038546488e-05\n",
            "Epoch: [62][0/55]\tTime 0.725 (0.725)\tData 0.546 (0.546)\tLoss 0.2683 (0.2683)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.522847167876085, Accuracy: (23.88663968383542)\n",
            "\n",
            "lr: 9.146101038546488e-05\n",
            "Epoch: [63][0/55]\tTime 0.758 (0.758)\tData 0.547 (0.547)\tLoss 0.1708 (0.1708)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.314097454673366, Accuracy: (23.48178138424028)\n",
            "\n",
            "lr: 9.146101038546488e-05\n",
            "Epoch: [64][0/55]\tTime 0.696 (0.696)\tData 0.551 (0.551)\tLoss 0.2348 (0.2348)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.702634039195443, Accuracy: (24.696356283025704)\n",
            "\n",
            "lr: 9.146101038546488e-05\n",
            "Epoch: [65][0/55]\tTime 0.774 (0.774)\tData 0.610 (0.610)\tLoss 0.0618 (0.0618)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.3471776136019935, Accuracy: (23.48178138424028)\n",
            "\n",
            "lr: 9.146101038546488e-05\n",
            "Epoch: [66][0/55]\tTime 0.731 (0.731)\tData 0.533 (0.533)\tLoss 0.0326 (0.0326)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.676402667273394, Accuracy: (24.898785417379155)\n",
            "\n",
            "lr: 9.146101038546488e-05\n",
            "Epoch: [67][0/55]\tTime 0.731 (0.731)\tData 0.577 (0.577)\tLoss 0.3192 (0.3192)\tPrec@1 90.625 (90.625)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.36021339941604, Accuracy: (23.48178138424028)\n",
            "\n",
            "lr: 9.146101038546488e-05\n",
            "Epoch: [68][0/55]\tTime 0.709 (0.709)\tData 0.553 (0.553)\tLoss 0.2446 (0.2446)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.583345556066104, Accuracy: (26.31578948140627)\n",
            "\n",
            "lr: 9.146101038546488e-05\n",
            "Epoch: [69][0/55]\tTime 0.712 (0.712)\tData 0.545 (0.545)\tLoss 0.6818 (0.6818)\tPrec@1 93.750 (93.750)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.858247204830772, Accuracy: (25.708502032013556)\n",
            "\n",
            "lr: 4.182558210365071e-05\n",
            "Change lr\n",
            "Epoch: [70][0/55]\tTime 0.780 (0.780)\tData 0.579 (0.579)\tLoss 0.0761 (0.0761)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.719589874329355, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 4.182558210365071e-05\n",
            "Epoch: [71][0/55]\tTime 0.736 (0.736)\tData 0.535 (0.535)\tLoss 0.1395 (0.1395)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.566182514916547, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 4.182558210365071e-05\n",
            "Epoch: [72][0/55]\tTime 0.736 (0.736)\tData 0.586 (0.586)\tLoss 0.0654 (0.0654)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.629284719706547, Accuracy: (25.303643732418415)\n",
            "\n",
            "lr: 4.182558210365071e-05\n",
            "Epoch: [73][0/55]\tTime 0.764 (0.764)\tData 0.572 (0.572)\tLoss 0.1617 (0.1617)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.422861079938016, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 4.182558210365071e-05\n",
            "Epoch: [74][0/55]\tTime 0.789 (0.789)\tData 0.575 (0.575)\tLoss 0.2406 (0.2406)\tPrec@1 93.750 (93.750)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.635053348927363, Accuracy: (26.1133603316087)\n",
            "\n",
            "lr: 4.182558210365071e-05\n",
            "Epoch: [75][0/55]\tTime 0.812 (0.812)\tData 0.556 (0.556)\tLoss 0.1724 (0.1724)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.579695276885863, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 4.182558210365071e-05\n",
            "Epoch: [76][0/55]\tTime 0.742 (0.742)\tData 0.542 (0.542)\tLoss 0.1011 (0.1011)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.616159856078113, Accuracy: (25.708502032013556)\n",
            "\n",
            "lr: 4.182558210365071e-05\n",
            "Epoch: [77][0/55]\tTime 0.736 (0.736)\tData 0.542 (0.542)\tLoss 0.1295 (0.1295)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.846625111846306, Accuracy: (26.92307693079898)\n",
            "\n",
            "lr: 4.182558210365071e-05\n",
            "Epoch: [78][0/55]\tTime 0.729 (0.729)\tData 0.532 (0.532)\tLoss 0.1731 (0.1731)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.653616395556492, Accuracy: (26.92307693079898)\n",
            "\n",
            "lr: 4.182558210365071e-05\n",
            "Epoch: [79][0/55]\tTime 0.779 (0.779)\tData 0.555 (0.555)\tLoss 0.0297 (0.0297)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.851685786536831, Accuracy: (26.31578948140627)\n",
            "\n",
            "lr: 1.9127049995800626e-05\n",
            "Change lr\n",
            "Epoch: [80][0/55]\tTime 0.776 (0.776)\tData 0.565 (0.565)\tLoss 0.1243 (0.1243)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.775789299474554, Accuracy: (26.1133603316087)\n",
            "\n",
            "lr: 1.9127049995800626e-05\n",
            "Epoch: [81][0/55]\tTime 0.781 (0.781)\tData 0.563 (0.563)\tLoss 0.0685 (0.0685)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.799550446421511, Accuracy: (26.72064778100141)\n",
            "\n",
            "lr: 1.9127049995800626e-05\n",
            "Epoch: [82][0/55]\tTime 0.768 (0.768)\tData 0.545 (0.545)\tLoss 0.2088 (0.2088)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.8206791202066395, Accuracy: (26.1133603316087)\n",
            "\n",
            "lr: 1.9127049995800626e-05\n",
            "Epoch: [83][0/55]\tTime 0.741 (0.741)\tData 0.566 (0.566)\tLoss 0.1791 (0.1791)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.809168240319379, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.9127049995800626e-05\n",
            "Epoch: [84][0/55]\tTime 0.696 (0.696)\tData 0.545 (0.545)\tLoss 0.0745 (0.0745)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.7425070310893815, Accuracy: (26.51821863120384)\n",
            "\n",
            "lr: 1.9127049995800626e-05\n",
            "Epoch: [85][0/55]\tTime 0.743 (0.743)\tData 0.564 (0.564)\tLoss 0.1557 (0.1557)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.709894558678754, Accuracy: (26.51821863120384)\n",
            "\n",
            "lr: 1.9127049995800626e-05\n",
            "Epoch: [86][0/55]\tTime 0.744 (0.744)\tData 0.539 (0.539)\tLoss 0.2807 (0.2807)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.781045960028645, Accuracy: (25.506072882215985)\n",
            "\n",
            "lr: 1.9127049995800626e-05\n",
            "Epoch: [87][0/55]\tTime 0.732 (0.732)\tData 0.550 (0.550)\tLoss 0.4457 (0.4457)\tPrec@1 96.875 (96.875)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.70967430238299, Accuracy: (26.31578948140627)\n",
            "\n",
            "lr: 1.9127049995800626e-05\n",
            "Epoch: [88][0/55]\tTime 0.745 (0.745)\tData 0.551 (0.551)\tLoss 0.0545 (0.0545)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.667083512433627, Accuracy: (26.1133603316087)\n",
            "\n",
            "lr: 1.9127049995800626e-05\n",
            "Epoch: [89][0/55]\tTime 0.775 (0.775)\tData 0.598 (0.598)\tLoss 0.4560 (0.4560)\tPrec@1 90.625 (90.625)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.733385865987554, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 8.746896591546167e-06\n",
            "Change lr\n",
            "Epoch: [90][0/55]\tTime 0.807 (0.807)\tData 0.594 (0.594)\tLoss 0.0385 (0.0385)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.740722250841889, Accuracy: (25.708502032013556)\n",
            "\n",
            "lr: 8.746896591546167e-06\n",
            "Epoch: [91][0/55]\tTime 0.727 (0.727)\tData 0.545 (0.545)\tLoss 0.1671 (0.1671)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.786636510841277, Accuracy: (26.1133603316087)\n",
            "\n",
            "lr: 8.746896591546167e-06\n",
            "Epoch: [92][0/55]\tTime 0.767 (0.767)\tData 0.560 (0.560)\tLoss 0.1343 (0.1343)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.752143296152957, Accuracy: (25.303643732418415)\n",
            "\n",
            "lr: 8.746896591546167e-06\n",
            "Epoch: [93][0/55]\tTime 0.765 (0.765)\tData 0.600 (0.600)\tLoss 0.1214 (0.1214)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.78304040190662, Accuracy: (25.303643732418415)\n",
            "\n",
            "lr: 8.746896591546167e-06\n",
            "Epoch: [94][0/55]\tTime 0.742 (0.742)\tData 0.548 (0.548)\tLoss 0.0765 (0.0765)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.822940687418949, Accuracy: (25.506072882215985)\n",
            "\n",
            "lr: 8.746896591546167e-06\n",
            "Epoch: [95][0/55]\tTime 0.751 (0.751)\tData 0.558 (0.558)\tLoss 0.1042 (0.1042)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.797710275843076, Accuracy: (25.708502032013556)\n",
            "\n",
            "lr: 8.746896591546167e-06\n",
            "Epoch: [96][0/55]\tTime 0.798 (0.798)\tData 0.581 (0.581)\tLoss 0.1082 (0.1082)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.763519399079234, Accuracy: (25.506072882215985)\n",
            "\n",
            "lr: 8.746896591546167e-06\n",
            "Epoch: [97][0/55]\tTime 0.759 (0.759)\tData 0.546 (0.546)\tLoss 0.1714 (0.1714)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.798705406034523, Accuracy: (25.506072882215985)\n",
            "\n",
            "lr: 8.746896591546167e-06\n",
            "Epoch: [98][0/55]\tTime 0.729 (0.729)\tData 0.556 (0.556)\tLoss 0.1027 (0.1027)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.782829404359887, Accuracy: (25.708502032013556)\n",
            "\n",
            "lr: 8.746896591546167e-06\n",
            "Epoch: [99][0/55]\tTime 0.731 (0.731)\tData 0.568 (0.568)\tLoss 0.6599 (0.6599)\tPrec@1 93.750 (93.750)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.732752255582616, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.999999999999971e-06\n",
            "Change lr\n",
            "Epoch: [100][0/55]\tTime 0.736 (0.736)\tData 0.567 (0.567)\tLoss 0.1486 (0.1486)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.7346970098703975, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.999999999999971e-06\n",
            "Epoch: [101][0/55]\tTime 0.775 (0.775)\tData 0.563 (0.563)\tLoss 0.0761 (0.0761)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.732437848079543, Accuracy: (25.506072882215985)\n",
            "\n",
            "lr: 3.999999999999971e-06\n",
            "Epoch: [102][0/55]\tTime 0.734 (0.734)\tData 0.555 (0.555)\tLoss 1.3647 (1.3647)\tPrec@1 90.625 (90.625)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.717078768772635, Accuracy: (25.506072882215985)\n",
            "\n",
            "lr: 3.999999999999971e-06\n",
            "Epoch: [103][0/55]\tTime 0.763 (0.763)\tData 0.547 (0.547)\tLoss 0.1386 (0.1386)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.732370851493558, Accuracy: (25.506072882215985)\n",
            "\n",
            "lr: 3.999999999999971e-06\n",
            "Epoch: [104][0/55]\tTime 0.787 (0.787)\tData 0.584 (0.584)\tLoss 0.0415 (0.0415)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.7572747049061395, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.999999999999971e-06\n",
            "Epoch: [105][0/55]\tTime 0.788 (0.788)\tData 0.581 (0.581)\tLoss 0.0792 (0.0792)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.750719553063273, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.999999999999971e-06\n",
            "Epoch: [106][0/55]\tTime 0.741 (0.741)\tData 0.537 (0.537)\tLoss 0.0484 (0.0484)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.761848689090868, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.999999999999971e-06\n",
            "Epoch: [107][0/55]\tTime 0.761 (0.761)\tData 0.557 (0.557)\tLoss 0.8273 (0.8273)\tPrec@1 84.375 (84.375)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.759710199919789, Accuracy: (26.31578948140627)\n",
            "\n",
            "lr: 3.999999999999971e-06\n",
            "Epoch: [108][0/55]\tTime 0.749 (0.749)\tData 0.552 (0.552)\tLoss 0.0527 (0.0527)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.766940479819109, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.999999999999971e-06\n",
            "Epoch: [109][0/55]\tTime 0.716 (0.716)\tData 0.528 (0.528)\tLoss 0.2324 (0.2324)\tPrec@1 96.875 (96.875)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.773675713944532, Accuracy: (26.1133603316087)\n",
            "\n",
            "lr: 1.829220207709291e-06\n",
            "Change lr\n",
            "Epoch: [110][0/55]\tTime 0.757 (0.757)\tData 0.535 (0.535)\tLoss 0.0199 (0.0199)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.779193295158356, Accuracy: (26.1133603316087)\n",
            "\n",
            "lr: 1.829220207709291e-06\n",
            "Epoch: [111][0/55]\tTime 0.747 (0.747)\tData 0.545 (0.545)\tLoss 0.1247 (0.1247)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.778242010819285, Accuracy: (25.708502032013556)\n",
            "\n",
            "lr: 1.829220207709291e-06\n",
            "Epoch: [112][0/55]\tTime 0.757 (0.757)\tData 0.552 (0.552)\tLoss 0.0523 (0.0523)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.769395001986731, Accuracy: (25.506072882215985)\n",
            "\n",
            "lr: 1.829220207709291e-06\n",
            "Epoch: [113][0/55]\tTime 0.701 (0.701)\tData 0.518 (0.518)\tLoss 0.0618 (0.0618)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.770813633073197, Accuracy: (25.708502032013556)\n",
            "\n",
            "lr: 1.829220207709291e-06\n",
            "Epoch: [114][0/55]\tTime 0.784 (0.784)\tData 0.566 (0.566)\tLoss 0.1668 (0.1668)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.783309990577852, Accuracy: (26.31578948140627)\n",
            "\n",
            "lr: 1.829220207709291e-06\n",
            "Epoch: [115][0/55]\tTime 0.705 (0.705)\tData 0.529 (0.529)\tLoss 0.1489 (0.1489)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.769011339195344, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.829220207709291e-06\n",
            "Epoch: [116][0/55]\tTime 0.739 (0.739)\tData 0.569 (0.569)\tLoss 0.0468 (0.0468)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.776623517395514, Accuracy: (25.708502032013556)\n",
            "\n",
            "lr: 1.829220207709291e-06\n",
            "Epoch: [117][0/55]\tTime 0.749 (0.749)\tData 0.576 (0.576)\tLoss 0.1933 (0.1933)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.775377207922067, Accuracy: (25.506072882215985)\n",
            "\n",
            "lr: 1.829220207709291e-06\n",
            "Epoch: [118][0/55]\tTime 0.735 (0.735)\tData 0.565 (0.565)\tLoss 0.0482 (0.0482)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.783848573321755, Accuracy: (25.708502032013556)\n",
            "\n",
            "lr: 1.829220207709291e-06\n",
            "Epoch: [119][0/55]\tTime 0.778 (0.778)\tData 0.576 (0.576)\tLoss 0.1832 (0.1832)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.790051039413885, Accuracy: (25.708502032013556)\n",
            "\n",
            "lr: 8.365116420730112e-07\n",
            "Change lr\n",
            "Epoch: [120][0/55]\tTime 0.710 (0.710)\tData 0.519 (0.519)\tLoss 0.5710 (0.5710)\tPrec@1 93.750 (93.750)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.7916392585043965, Accuracy: (25.708502032013556)\n",
            "\n",
            "lr: 8.365116420730112e-07\n",
            "Epoch: [121][0/55]\tTime 0.756 (0.756)\tData 0.559 (0.559)\tLoss 0.0541 (0.0541)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.79465634040987, Accuracy: (25.506072882215985)\n",
            "\n",
            "lr: 8.365116420730112e-07\n",
            "Epoch: [122][0/55]\tTime 0.745 (0.745)\tData 0.548 (0.548)\tLoss 0.1366 (0.1366)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.797466220160728, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 8.365116420730112e-07\n",
            "Epoch: [123][0/55]\tTime 0.750 (0.750)\tData 0.581 (0.581)\tLoss 0.0249 (0.0249)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.798808086256266, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 8.365116420730112e-07\n",
            "Epoch: [124][0/55]\tTime 0.695 (0.695)\tData 0.521 (0.521)\tLoss 0.1412 (0.1412)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.799282487104779, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 8.365116420730112e-07\n",
            "Epoch: [125][0/55]\tTime 0.748 (0.748)\tData 0.544 (0.544)\tLoss 0.3059 (0.3059)\tPrec@1 90.625 (90.625)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.794140533879701, Accuracy: (25.708502032013556)\n",
            "\n",
            "lr: 8.365116420730112e-07\n",
            "Epoch: [126][0/55]\tTime 0.701 (0.701)\tData 0.546 (0.546)\tLoss 0.0542 (0.0542)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.792009805378161, Accuracy: (25.708502032013556)\n",
            "\n",
            "lr: 8.365116420730112e-07\n",
            "Epoch: [127][0/55]\tTime 0.744 (0.744)\tData 0.548 (0.548)\tLoss 0.1694 (0.1694)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.791555617019715, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 8.365116420730112e-07\n",
            "Epoch: [128][0/55]\tTime 0.771 (0.771)\tData 0.577 (0.577)\tLoss 0.2103 (0.2103)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.791996515714205, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 8.365116420730112e-07\n",
            "Epoch: [129][0/55]\tTime 0.734 (0.734)\tData 0.539 (0.539)\tLoss 0.0908 (0.0908)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.792879660602523, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.825409999160112e-07\n",
            "Change lr\n",
            "Epoch: [130][0/55]\tTime 0.701 (0.701)\tData 0.525 (0.525)\tLoss 0.1039 (0.1039)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.791554099635074, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.825409999160112e-07\n",
            "Epoch: [131][0/55]\tTime 0.729 (0.729)\tData 0.563 (0.563)\tLoss 0.1369 (0.1369)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.791160649133597, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.825409999160112e-07\n",
            "Epoch: [132][0/55]\tTime 0.791 (0.791)\tData 0.573 (0.573)\tLoss 0.0442 (0.0442)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.790937860002402, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.825409999160112e-07\n",
            "Epoch: [133][0/55]\tTime 0.768 (0.768)\tData 0.559 (0.559)\tLoss 0.5546 (0.5546)\tPrec@1 93.750 (93.750)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.792360780692777, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.825409999160112e-07\n",
            "Epoch: [134][0/55]\tTime 0.753 (0.753)\tData 0.545 (0.545)\tLoss 0.2082 (0.2082)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.791389040618773, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.825409999160112e-07\n",
            "Epoch: [135][0/55]\tTime 0.734 (0.734)\tData 0.542 (0.542)\tLoss 0.0964 (0.0964)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.792740578593513, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.825409999160112e-07\n",
            "Epoch: [136][0/55]\tTime 0.721 (0.721)\tData 0.546 (0.546)\tLoss 0.0443 (0.0443)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.793503568240022, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.825409999160112e-07\n",
            "Epoch: [137][0/55]\tTime 0.725 (0.725)\tData 0.547 (0.547)\tLoss 0.1583 (0.1583)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.79451531244193, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.825409999160112e-07\n",
            "Epoch: [138][0/55]\tTime 0.782 (0.782)\tData 0.570 (0.570)\tLoss 0.0300 (0.0300)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.792699199939063, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.825409999160112e-07\n",
            "Epoch: [139][0/55]\tTime 0.753 (0.753)\tData 0.555 (0.555)\tLoss 0.0677 (0.0677)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.790102394968875, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.749379318309227e-07\n",
            "Change lr\n",
            "Epoch: [140][0/55]\tTime 0.752 (0.752)\tData 0.557 (0.557)\tLoss 0.0718 (0.0718)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.790286095036186, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.749379318309227e-07\n",
            "Epoch: [141][0/55]\tTime 0.780 (0.780)\tData 0.579 (0.579)\tLoss 0.1496 (0.1496)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.790450493816422, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.749379318309227e-07\n",
            "Epoch: [142][0/55]\tTime 0.733 (0.733)\tData 0.555 (0.555)\tLoss 0.0865 (0.0865)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789820767607283, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.749379318309227e-07\n",
            "Epoch: [143][0/55]\tTime 0.776 (0.776)\tData 0.575 (0.575)\tLoss 0.0270 (0.0270)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789387741552191, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.749379318309227e-07\n",
            "Epoch: [144][0/55]\tTime 0.757 (0.757)\tData 0.545 (0.545)\tLoss 0.0322 (0.0322)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789674206783897, Accuracy: (25.708502032013556)\n",
            "\n",
            "lr: 1.749379318309227e-07\n",
            "Epoch: [145][0/55]\tTime 0.799 (0.799)\tData 0.648 (0.648)\tLoss 0.1667 (0.1667)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.790314844262745, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.749379318309227e-07\n",
            "Epoch: [146][0/55]\tTime 0.786 (0.786)\tData 0.557 (0.557)\tLoss 0.1160 (0.1160)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789799335031857, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.749379318309227e-07\n",
            "Epoch: [147][0/55]\tTime 0.748 (0.748)\tData 0.574 (0.574)\tLoss 0.0958 (0.0958)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789450340425438, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.749379318309227e-07\n",
            "Epoch: [148][0/55]\tTime 0.724 (0.724)\tData 0.542 (0.542)\tLoss 0.5909 (0.5909)\tPrec@1 93.750 (93.750)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.788891081867913, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.749379318309227e-07\n",
            "Epoch: [149][0/55]\tTime 0.749 (0.749)\tData 0.567 (0.567)\tLoss 0.0933 (0.0933)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789226219239023, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.999999999999911e-08\n",
            "Change lr\n",
            "Epoch: [150][0/55]\tTime 0.785 (0.785)\tData 0.595 (0.595)\tLoss 0.0143 (0.0143)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789258509029744, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.999999999999911e-08\n",
            "Epoch: [151][0/55]\tTime 0.746 (0.746)\tData 0.557 (0.557)\tLoss 0.1433 (0.1433)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.7892851810223664, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.999999999999911e-08\n",
            "Epoch: [152][0/55]\tTime 0.716 (0.716)\tData 0.549 (0.549)\tLoss 0.0554 (0.0554)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.7896487703207535, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.999999999999911e-08\n",
            "Epoch: [153][0/55]\tTime 0.760 (0.760)\tData 0.565 (0.565)\tLoss 0.0848 (0.0848)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789446749667889, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.999999999999911e-08\n",
            "Epoch: [154][0/55]\tTime 0.700 (0.700)\tData 0.528 (0.528)\tLoss 0.0987 (0.0987)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.7891723771809565, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.999999999999911e-08\n",
            "Epoch: [155][0/55]\tTime 0.755 (0.755)\tData 0.551 (0.551)\tLoss 0.0387 (0.0387)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.78934902700818, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.999999999999911e-08\n",
            "Epoch: [156][0/55]\tTime 0.754 (0.754)\tData 0.558 (0.558)\tLoss 1.4184 (1.4184)\tPrec@1 90.625 (90.625)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.78944246778604, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.999999999999911e-08\n",
            "Epoch: [157][0/55]\tTime 0.753 (0.753)\tData 0.558 (0.558)\tLoss 0.1368 (0.1368)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.790109352544252, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.999999999999911e-08\n",
            "Epoch: [158][0/55]\tTime 0.768 (0.768)\tData 0.549 (0.549)\tLoss 0.0908 (0.0908)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.790186526804318, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.999999999999911e-08\n",
            "Epoch: [159][0/55]\tTime 0.720 (0.720)\tData 0.551 (0.551)\tLoss 0.0236 (0.0236)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789919926570012, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.658440415418568e-08\n",
            "Change lr\n",
            "Epoch: [160][0/55]\tTime 0.752 (0.752)\tData 0.552 (0.552)\tLoss 0.0600 (0.0600)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789805365960125, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.658440415418568e-08\n",
            "Epoch: [161][0/55]\tTime 0.710 (0.710)\tData 0.516 (0.516)\tLoss 0.1752 (0.1752)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789762643667368, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.658440415418568e-08\n",
            "Epoch: [162][0/55]\tTime 0.769 (0.769)\tData 0.562 (0.562)\tLoss 0.5760 (0.5760)\tPrec@1 93.750 (93.750)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.789812682611257, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.658440415418568e-08\n",
            "Epoch: [163][0/55]\tTime 0.737 (0.737)\tData 0.525 (0.525)\tLoss 0.0600 (0.0600)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789778076202763, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.658440415418568e-08\n",
            "Epoch: [164][0/55]\tTime 0.733 (0.733)\tData 0.540 (0.540)\tLoss 0.2303 (0.2303)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789552626822159, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.658440415418568e-08\n",
            "Epoch: [165][0/55]\tTime 0.732 (0.732)\tData 0.552 (0.552)\tLoss 0.2855 (0.2855)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789581275661948, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.658440415418568e-08\n",
            "Epoch: [166][0/55]\tTime 0.754 (0.754)\tData 0.564 (0.564)\tLoss 0.1920 (0.1920)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789514881396583, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.658440415418568e-08\n",
            "Epoch: [167][0/55]\tTime 0.705 (0.705)\tData 0.528 (0.528)\tLoss 0.0928 (0.0928)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789530537871697, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.658440415418568e-08\n",
            "Epoch: [168][0/55]\tTime 0.771 (0.771)\tData 0.556 (0.556)\tLoss 0.0729 (0.0729)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789620739245704, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.658440415418568e-08\n",
            "Epoch: [169][0/55]\tTime 0.726 (0.726)\tData 0.548 (0.548)\tLoss 0.0461 (0.0461)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789535954896255, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.6730232841460165e-08\n",
            "Change lr\n",
            "Epoch: [170][0/55]\tTime 0.736 (0.736)\tData 0.535 (0.535)\tLoss 0.1490 (0.1490)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.7895379278823915, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.6730232841460165e-08\n",
            "Epoch: [171][0/55]\tTime 0.761 (0.761)\tData 0.583 (0.583)\tLoss 0.2284 (0.2284)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789512317672915, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.6730232841460165e-08\n",
            "Epoch: [172][0/55]\tTime 0.805 (0.805)\tData 0.644 (0.644)\tLoss 0.1645 (0.1645)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789544352635681, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.6730232841460165e-08\n",
            "Epoch: [173][0/55]\tTime 0.749 (0.749)\tData 0.556 (0.556)\tLoss 0.1554 (0.1554)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.7895163601709285, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.6730232841460165e-08\n",
            "Epoch: [174][0/55]\tTime 0.713 (0.713)\tData 0.539 (0.539)\tLoss 0.1477 (0.1477)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789502228802515, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.6730232841460165e-08\n",
            "Epoch: [175][0/55]\tTime 0.723 (0.723)\tData 0.546 (0.546)\tLoss 0.0387 (0.0387)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.78950655315569, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.6730232841460165e-08\n",
            "Epoch: [176][0/55]\tTime 0.736 (0.736)\tData 0.580 (0.580)\tLoss 0.1243 (0.1243)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789484680422887, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.6730232841460165e-08\n",
            "Epoch: [177][0/55]\tTime 0.776 (0.776)\tData 0.546 (0.546)\tLoss 0.1316 (0.1316)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789480321320445, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.6730232841460165e-08\n",
            "Epoch: [178][0/55]\tTime 0.769 (0.769)\tData 0.550 (0.550)\tLoss 0.2750 (0.2750)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789449819186438, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.6730232841460165e-08\n",
            "Epoch: [179][0/55]\tTime 0.739 (0.739)\tData 0.558 (0.558)\tLoss 1.0859 (1.0859)\tPrec@1 93.750 (93.750)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.789465240138745, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.650819998320195e-09\n",
            "Change lr\n",
            "Epoch: [180][0/55]\tTime 0.778 (0.778)\tData 0.602 (0.602)\tLoss 0.1738 (0.1738)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789461020033369, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.650819998320195e-09\n",
            "Epoch: [181][0/55]\tTime 0.725 (0.725)\tData 0.538 (0.538)\tLoss 0.1483 (0.1483)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789464182216629, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.650819998320195e-09\n",
            "Epoch: [182][0/55]\tTime 0.772 (0.772)\tData 0.594 (0.594)\tLoss 0.4077 (0.4077)\tPrec@1 93.750 (93.750)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.789462687998165, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.650819998320195e-09\n",
            "Epoch: [183][0/55]\tTime 0.731 (0.731)\tData 0.520 (0.520)\tLoss 0.1507 (0.1507)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789461633937079, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.650819998320195e-09\n",
            "Epoch: [184][0/55]\tTime 0.722 (0.722)\tData 0.546 (0.546)\tLoss 0.1116 (0.1116)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.7894565296559195, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.650819998320195e-09\n",
            "Epoch: [185][0/55]\tTime 0.779 (0.779)\tData 0.545 (0.545)\tLoss 0.0343 (0.0343)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789447085577467, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.650819998320195e-09\n",
            "Epoch: [186][0/55]\tTime 0.771 (0.771)\tData 0.558 (0.558)\tLoss 0.0829 (0.0829)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432448414173, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.650819998320195e-09\n",
            "Epoch: [187][0/55]\tTime 0.714 (0.714)\tData 0.536 (0.536)\tLoss 0.1433 (0.1433)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789433035290676, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.650819998320195e-09\n",
            "Epoch: [188][0/55]\tTime 0.772 (0.772)\tData 0.531 (0.531)\tLoss 0.1220 (0.1220)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789427633710235, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.650819998320195e-09\n",
            "Epoch: [189][0/55]\tTime 0.758 (0.758)\tData 0.571 (0.571)\tLoss 0.0382 (0.0382)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789437537251214, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.498758636618441e-09\n",
            "Change lr\n",
            "Epoch: [190][0/55]\tTime 0.797 (0.797)\tData 0.555 (0.555)\tLoss 0.7098 (0.7098)\tPrec@1 93.750 (93.750)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.789437938798295, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.498758636618441e-09\n",
            "Epoch: [191][0/55]\tTime 0.784 (0.784)\tData 0.551 (0.551)\tLoss 0.1167 (0.1167)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789440259277097, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.498758636618441e-09\n",
            "Epoch: [192][0/55]\tTime 0.805 (0.805)\tData 0.601 (0.601)\tLoss 0.0225 (0.0225)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789439340352047, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.498758636618441e-09\n",
            "Epoch: [193][0/55]\tTime 0.773 (0.773)\tData 0.562 (0.562)\tLoss 0.0622 (0.0622)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789437606749747, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.498758636618441e-09\n",
            "Epoch: [194][0/55]\tTime 0.712 (0.712)\tData 0.557 (0.557)\tLoss 0.1585 (0.1585)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.78943747547474, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.498758636618441e-09\n",
            "Epoch: [195][0/55]\tTime 0.719 (0.719)\tData 0.540 (0.540)\tLoss 0.1380 (0.1380)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789437857716672, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.498758636618441e-09\n",
            "Epoch: [196][0/55]\tTime 0.760 (0.760)\tData 0.553 (0.553)\tLoss 0.0548 (0.0548)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789437351921792, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.498758636618441e-09\n",
            "Epoch: [197][0/55]\tTime 0.754 (0.754)\tData 0.568 (0.568)\tLoss 0.0319 (0.0319)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789437228368844, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.498758636618441e-09\n",
            "Epoch: [198][0/55]\tTime 0.756 (0.756)\tData 0.550 (0.550)\tLoss 0.0841 (0.0841)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789435888591566, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.498758636618441e-09\n",
            "Epoch: [199][0/55]\tTime 0.747 (0.747)\tData 0.580 (0.580)\tLoss 0.1487 (0.1487)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432517912706, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.5999999999999765e-09\n",
            "Change lr\n",
            "Epoch: [200][0/55]\tTime 0.737 (0.737)\tData 0.543 (0.543)\tLoss 0.0353 (0.0353)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432691659039, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.5999999999999765e-09\n",
            "Epoch: [201][0/55]\tTime 0.780 (0.780)\tData 0.593 (0.593)\tLoss 0.0455 (0.0455)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432452275203, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.5999999999999765e-09\n",
            "Epoch: [202][0/55]\tTime 0.735 (0.735)\tData 0.537 (0.537)\tLoss 0.0481 (0.0481)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432564245062, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.5999999999999765e-09\n",
            "Epoch: [203][0/55]\tTime 0.770 (0.770)\tData 0.571 (0.571)\tLoss 0.1176 (0.1176)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789433209037009, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.5999999999999765e-09\n",
            "Epoch: [204][0/55]\tTime 0.736 (0.736)\tData 0.552 (0.552)\tLoss 0.0500 (0.0500)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789433479309082, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.5999999999999765e-09\n",
            "Epoch: [205][0/55]\tTime 0.740 (0.740)\tData 0.557 (0.557)\tLoss 0.1349 (0.1349)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.7894335024752595, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.5999999999999765e-09\n",
            "Epoch: [206][0/55]\tTime 0.739 (0.739)\tData 0.546 (0.546)\tLoss 0.0460 (0.0460)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432919459787, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.5999999999999765e-09\n",
            "Epoch: [207][0/55]\tTime 0.689 (0.689)\tData 0.533 (0.533)\tLoss 0.1065 (0.1065)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432452275203, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.5999999999999765e-09\n",
            "Epoch: [208][0/55]\tTime 0.777 (0.777)\tData 0.567 (0.567)\tLoss 0.0196 (0.0196)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432548800943, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.5999999999999765e-09\n",
            "Epoch: [209][0/55]\tTime 0.737 (0.737)\tData 0.550 (0.550)\tLoss 0.2677 (0.2677)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.78943227466784, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.316880830837109e-10\n",
            "Change lr\n",
            "Epoch: [210][0/55]\tTime 0.798 (0.798)\tData 0.586 (0.586)\tLoss 1.0577 (1.0577)\tPrec@1 96.875 (96.875)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.7894324986075585, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.316880830837109e-10\n",
            "Epoch: [211][0/55]\tTime 0.713 (0.713)\tData 0.548 (0.548)\tLoss 0.0560 (0.0560)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432487024469, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.316880830837109e-10\n",
            "Epoch: [212][0/55]\tTime 0.716 (0.716)\tData 0.547 (0.547)\tLoss 0.0685 (0.0685)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432668492862, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.316880830837109e-10\n",
            "Epoch: [213][0/55]\tTime 0.765 (0.765)\tData 0.586 (0.586)\tLoss 0.0149 (0.0149)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.7894327572965425, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.316880830837109e-10\n",
            "Epoch: [214][0/55]\tTime 0.730 (0.730)\tData 0.565 (0.565)\tLoss 0.0638 (0.0638)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432606716388, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.316880830837109e-10\n",
            "Epoch: [215][0/55]\tTime 0.718 (0.718)\tData 0.542 (0.542)\tLoss 0.0932 (0.0932)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432575828151, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.316880830837109e-10\n",
            "Epoch: [216][0/55]\tTime 0.701 (0.701)\tData 0.532 (0.532)\tLoss 0.1284 (0.1284)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432367332551, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.316880830837109e-10\n",
            "Epoch: [217][0/55]\tTime 0.765 (0.765)\tData 0.545 (0.545)\tLoss 0.0517 (0.0517)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432575828151, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.316880830837109e-10\n",
            "Epoch: [218][0/55]\tTime 0.742 (0.742)\tData 0.537 (0.537)\tLoss 0.0724 (0.0724)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432336444314, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 7.316880830837109e-10\n",
            "Epoch: [219][0/55]\tTime 0.768 (0.768)\tData 0.570 (0.570)\tLoss 0.1376 (0.1376)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432602855358, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.3460465682920205e-10\n",
            "Change lr\n",
            "Epoch: [220][0/55]\tTime 0.779 (0.779)\tData 0.549 (0.549)\tLoss 0.0404 (0.0404)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432459997262, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.3460465682920205e-10\n",
            "Epoch: [221][0/55]\tTime 0.754 (0.754)\tData 0.578 (0.578)\tLoss 0.0911 (0.0911)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432664631832, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.3460465682920205e-10\n",
            "Epoch: [222][0/55]\tTime 0.768 (0.768)\tData 0.599 (0.599)\tLoss 1.3880 (1.3880)\tPrec@1 93.750 (93.750)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.789432452275203, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.3460465682920205e-10\n",
            "Epoch: [223][0/55]\tTime 0.733 (0.733)\tData 0.530 (0.530)\tLoss 0.2156 (0.2156)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432672353891, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.3460465682920205e-10\n",
            "Epoch: [224][0/55]\tTime 0.710 (0.710)\tData 0.560 (0.560)\tLoss 0.1410 (0.1410)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432417525936, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.3460465682920205e-10\n",
            "Epoch: [225][0/55]\tTime 0.778 (0.778)\tData 0.553 (0.553)\tLoss 0.0662 (0.0662)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432548800943, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.3460465682920205e-10\n",
            "Epoch: [226][0/55]\tTime 0.734 (0.734)\tData 0.554 (0.554)\tLoss 0.1941 (0.1941)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432405942847, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.3460465682920205e-10\n",
            "Epoch: [227][0/55]\tTime 0.749 (0.749)\tData 0.562 (0.562)\tLoss 0.0200 (0.0200)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432452275203, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.3460465682920205e-10\n",
            "Epoch: [228][0/55]\tTime 0.797 (0.797)\tData 0.596 (0.596)\tLoss 0.1015 (0.1015)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432730269335, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.3460465682920205e-10\n",
            "Epoch: [229][0/55]\tTime 0.748 (0.748)\tData 0.562 (0.562)\tLoss 0.2657 (0.2657)\tPrec@1 96.875 (96.875)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.789432510190647, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.5301639996640336e-10\n",
            "Change lr\n",
            "Epoch: [230][0/55]\tTime 0.801 (0.801)\tData 0.616 (0.616)\tLoss 0.2036 (0.2036)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432112504596, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.5301639996640336e-10\n",
            "Epoch: [231][0/55]\tTime 0.783 (0.783)\tData 0.594 (0.594)\tLoss 0.0453 (0.0453)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789433073900971, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.5301639996640336e-10\n",
            "Epoch: [232][0/55]\tTime 0.727 (0.727)\tData 0.540 (0.540)\tLoss 0.0826 (0.0826)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789431934897234, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.5301639996640336e-10\n",
            "Epoch: [233][0/55]\tTime 0.744 (0.744)\tData 0.539 (0.539)\tLoss 0.0604 (0.0604)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432243779603, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.5301639996640336e-10\n",
            "Epoch: [234][0/55]\tTime 0.755 (0.755)\tData 0.544 (0.544)\tLoss 0.1243 (0.1243)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432637604625, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.5301639996640336e-10\n",
            "Epoch: [235][0/55]\tTime 0.761 (0.761)\tData 0.567 (0.567)\tLoss 0.2056 (0.2056)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432417525936, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.5301639996640336e-10\n",
            "Epoch: [236][0/55]\tTime 0.703 (0.703)\tData 0.528 (0.528)\tLoss 0.3372 (0.3372)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.7894325140516765, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.5301639996640336e-10\n",
            "Epoch: [237][0/55]\tTime 0.692 (0.692)\tData 0.516 (0.516)\tLoss 0.7480 (0.7480)\tPrec@1 90.625 (90.625)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.789432521773736, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.5301639996640336e-10\n",
            "Epoch: [238][0/55]\tTime 0.723 (0.723)\tData 0.540 (0.540)\tLoss 0.1344 (0.1344)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.7894323557494625, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.5301639996640336e-10\n",
            "Epoch: [239][0/55]\tTime 0.729 (0.729)\tData 0.548 (0.548)\tLoss 0.0105 (0.0105)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.78943288857155, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.997517273236857e-11\n",
            "Change lr\n",
            "Epoch: [240][0/55]\tTime 0.802 (0.802)\tData 0.582 (0.582)\tLoss 0.0968 (0.0968)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432907876698, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.997517273236857e-11\n",
            "Epoch: [241][0/55]\tTime 0.772 (0.772)\tData 0.598 (0.598)\tLoss 0.0664 (0.0664)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432390498729, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.997517273236857e-11\n",
            "Epoch: [242][0/55]\tTime 0.799 (0.799)\tData 0.591 (0.591)\tLoss 1.0805 (1.0805)\tPrec@1 90.625 (90.625)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.789432730269335, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.997517273236857e-11\n",
            "Epoch: [243][0/55]\tTime 0.780 (0.780)\tData 0.557 (0.557)\tLoss 0.0552 (0.0552)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432390498729, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.997517273236857e-11\n",
            "Epoch: [244][0/55]\tTime 0.750 (0.750)\tData 0.540 (0.540)\tLoss 0.0906 (0.0906)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432251501663, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.997517273236857e-11\n",
            "Epoch: [245][0/55]\tTime 0.804 (0.804)\tData 0.585 (0.585)\tLoss 0.0807 (0.0807)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432691659039, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.997517273236857e-11\n",
            "Epoch: [246][0/55]\tTime 0.811 (0.811)\tData 0.577 (0.577)\tLoss 0.0846 (0.0846)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432297834018, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.997517273236857e-11\n",
            "Epoch: [247][0/55]\tTime 0.745 (0.745)\tData 0.559 (0.559)\tLoss 1.9767 (1.9767)\tPrec@1 93.750 (93.750)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.789432610577418, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.997517273236857e-11\n",
            "Epoch: [248][0/55]\tTime 0.744 (0.744)\tData 0.581 (0.581)\tLoss 0.2296 (0.2296)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432487024469, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.997517273236857e-11\n",
            "Epoch: [249][0/55]\tTime 0.739 (0.739)\tData 0.556 (0.556)\tLoss 0.0741 (0.0741)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.7894323982207885, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.199999999999942e-11\n",
            "Change lr\n",
            "Epoch: [250][0/55]\tTime 0.701 (0.701)\tData 0.554 (0.554)\tLoss 0.2785 (0.2785)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432263084751, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.199999999999942e-11\n",
            "Epoch: [251][0/55]\tTime 0.761 (0.761)\tData 0.578 (0.578)\tLoss 0.0326 (0.0326)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432363471522, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.199999999999942e-11\n",
            "Epoch: [252][0/55]\tTime 0.754 (0.754)\tData 0.562 (0.562)\tLoss 0.0765 (0.0765)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432637604625, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.199999999999942e-11\n",
            "Epoch: [253][0/55]\tTime 0.735 (0.735)\tData 0.572 (0.572)\tLoss 0.0327 (0.0327)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432328722255, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.199999999999942e-11\n",
            "Epoch: [254][0/55]\tTime 0.794 (0.794)\tData 0.564 (0.564)\tLoss 0.1672 (0.1672)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.78943218200313, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.199999999999942e-11\n",
            "Epoch: [255][0/55]\tTime 0.759 (0.759)\tData 0.564 (0.564)\tLoss 0.0951 (0.0951)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432548800943, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.199999999999942e-11\n",
            "Epoch: [256][0/55]\tTime 0.775 (0.775)\tData 0.574 (0.574)\tLoss 1.1828 (1.1828)\tPrec@1 96.875 (96.875)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.789432394359759, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.199999999999942e-11\n",
            "Epoch: [257][0/55]\tTime 0.780 (0.780)\tData 0.560 (0.560)\tLoss 0.1055 (0.1055)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432792045809, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.199999999999942e-11\n",
            "Epoch: [258][0/55]\tTime 0.742 (0.742)\tData 0.576 (0.576)\tLoss 0.6228 (0.6228)\tPrec@1 96.875 (96.875)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.7894327572965425, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.199999999999942e-11\n",
            "Epoch: [259][0/55]\tTime 0.795 (0.795)\tData 0.631 (0.631)\tLoss 0.1876 (0.1876)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432610577418, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.4633761661674165e-11\n",
            "Change lr\n",
            "Epoch: [260][0/55]\tTime 0.732 (0.732)\tData 0.566 (0.566)\tLoss 0.0894 (0.0894)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432336444314, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.4633761661674165e-11\n",
            "Epoch: [261][0/55]\tTime 0.759 (0.759)\tData 0.584 (0.584)\tLoss 0.1114 (0.1114)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.78943227466784, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.4633761661674165e-11\n",
            "Epoch: [262][0/55]\tTime 0.760 (0.760)\tData 0.582 (0.582)\tLoss 0.1100 (0.1100)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432297834018, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.4633761661674165e-11\n",
            "Epoch: [263][0/55]\tTime 0.765 (0.765)\tData 0.577 (0.577)\tLoss 0.0585 (0.0585)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.7894324561362325, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.4633761661674165e-11\n",
            "Epoch: [264][0/55]\tTime 0.768 (0.768)\tData 0.574 (0.574)\tLoss 0.1148 (0.1148)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432359610492, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.4633761661674165e-11\n",
            "Epoch: [265][0/55]\tTime 0.764 (0.764)\tData 0.562 (0.562)\tLoss 0.1071 (0.1071)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.7894325140516765, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.4633761661674165e-11\n",
            "Epoch: [266][0/55]\tTime 0.723 (0.723)\tData 0.552 (0.552)\tLoss 0.0812 (0.0812)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.7894323557494625, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.4633761661674165e-11\n",
            "Epoch: [267][0/55]\tTime 0.730 (0.730)\tData 0.562 (0.562)\tLoss 0.1282 (0.1282)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432336444314, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.4633761661674165e-11\n",
            "Epoch: [268][0/55]\tTime 0.752 (0.752)\tData 0.564 (0.564)\tLoss 0.0098 (0.0098)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432390498729, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.4633761661674165e-11\n",
            "Epoch: [269][0/55]\tTime 0.799 (0.799)\tData 0.619 (0.619)\tLoss 0.0548 (0.0548)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789431961924441, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.6920931365840164e-12\n",
            "Change lr\n",
            "Epoch: [270][0/55]\tTime 0.756 (0.756)\tData 0.571 (0.571)\tLoss 0.0286 (0.0286)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432394359759, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.6920931365840164e-12\n",
            "Epoch: [271][0/55]\tTime 0.717 (0.717)\tData 0.566 (0.566)\tLoss 0.0861 (0.0861)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432487024469, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.6920931365840164e-12\n",
            "Epoch: [272][0/55]\tTime 0.775 (0.775)\tData 0.573 (0.573)\tLoss 0.2439 (0.2439)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432517912706, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.6920931365840164e-12\n",
            "Epoch: [273][0/55]\tTime 0.769 (0.769)\tData 0.559 (0.559)\tLoss 0.0711 (0.0711)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432548800943, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.6920931365840164e-12\n",
            "Epoch: [274][0/55]\tTime 0.791 (0.791)\tData 0.576 (0.576)\tLoss 0.0622 (0.0622)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.7894323982207885, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.6920931365840164e-12\n",
            "Epoch: [275][0/55]\tTime 0.744 (0.744)\tData 0.576 (0.576)\tLoss 0.0159 (0.0159)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.7894323982207885, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.6920931365840164e-12\n",
            "Epoch: [276][0/55]\tTime 0.733 (0.733)\tData 0.531 (0.531)\tLoss 0.2644 (0.2644)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432521773736, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.6920931365840164e-12\n",
            "Epoch: [277][0/55]\tTime 0.709 (0.709)\tData 0.538 (0.538)\tLoss 0.0265 (0.0265)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432637604625, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.6920931365840164e-12\n",
            "Epoch: [278][0/55]\tTime 0.711 (0.711)\tData 0.550 (0.550)\tLoss 0.0157 (0.0157)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432209030337, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.6920931365840164e-12\n",
            "Epoch: [279][0/55]\tTime 0.758 (0.758)\tData 0.555 (0.555)\tLoss 0.2347 (0.2347)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432301695047, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.0603279993280557e-12\n",
            "Change lr\n",
            "Epoch: [280][0/55]\tTime 0.783 (0.783)\tData 0.612 (0.612)\tLoss 0.1439 (0.1439)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432209030337, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.0603279993280557e-12\n",
            "Epoch: [281][0/55]\tTime 0.766 (0.766)\tData 0.559 (0.559)\tLoss 0.0823 (0.0823)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.78943227080681, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.0603279993280557e-12\n",
            "Epoch: [282][0/55]\tTime 0.744 (0.744)\tData 0.541 (0.541)\tLoss 0.0990 (0.0990)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.78943217428107, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.0603279993280557e-12\n",
            "Epoch: [283][0/55]\tTime 0.813 (0.813)\tData 0.596 (0.596)\tLoss 0.0824 (0.0824)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432205169307, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.0603279993280557e-12\n",
            "Epoch: [284][0/55]\tTime 0.781 (0.781)\tData 0.576 (0.576)\tLoss 0.0310 (0.0310)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432236057544, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.0603279993280557e-12\n",
            "Epoch: [285][0/55]\tTime 0.773 (0.773)\tData 0.560 (0.560)\tLoss 0.1629 (0.1629)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432266945781, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.0603279993280557e-12\n",
            "Epoch: [286][0/55]\tTime 0.810 (0.810)\tData 0.586 (0.586)\tLoss 0.0730 (0.0730)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.0603279993280557e-12\n",
            "Epoch: [287][0/55]\tTime 0.760 (0.760)\tData 0.550 (0.550)\tLoss 0.1504 (0.1504)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432209030337, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.0603279993280557e-12\n",
            "Epoch: [288][0/55]\tTime 0.750 (0.750)\tData 0.549 (0.549)\tLoss 0.1429 (0.1429)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 3.0603279993280557e-12\n",
            "Epoch: [289][0/55]\tTime 0.753 (0.753)\tData 0.574 (0.574)\tLoss 0.0827 (0.0827)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.7894321781421, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.3995034546473663e-12\n",
            "Change lr\n",
            "Epoch: [290][0/55]\tTime 0.817 (0.817)\tData 0.612 (0.612)\tLoss 0.0366 (0.0366)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.3995034546473663e-12\n",
            "Epoch: [291][0/55]\tTime 0.744 (0.744)\tData 0.593 (0.593)\tLoss 0.1186 (0.1186)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.3995034546473663e-12\n",
            "Epoch: [292][0/55]\tTime 0.741 (0.741)\tData 0.564 (0.564)\tLoss 0.0418 (0.0418)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.3995034546473663e-12\n",
            "Epoch: [293][0/55]\tTime 0.791 (0.791)\tData 0.614 (0.614)\tLoss 0.1984 (0.1984)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.3995034546473663e-12\n",
            "Epoch: [294][0/55]\tTime 0.789 (0.789)\tData 0.563 (0.563)\tLoss 0.0761 (0.0761)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.3995034546473663e-12\n",
            "Epoch: [295][0/55]\tTime 0.801 (0.801)\tData 0.598 (0.598)\tLoss 0.0298 (0.0298)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.3995034546473663e-12\n",
            "Epoch: [296][0/55]\tTime 0.836 (0.836)\tData 0.659 (0.659)\tLoss 0.1391 (0.1391)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.3995034546473663e-12\n",
            "Epoch: [297][0/55]\tTime 0.802 (0.802)\tData 0.605 (0.605)\tLoss 0.1648 (0.1648)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.3995034546473663e-12\n",
            "Epoch: [298][0/55]\tTime 0.755 (0.755)\tData 0.573 (0.573)\tLoss 0.1034 (0.1034)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.3995034546473663e-12\n",
            "Epoch: [299][0/55]\tTime 0.799 (0.799)\tData 0.591 (0.591)\tLoss 0.0816 (0.0816)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.399999999999859e-13\n",
            "Change lr\n",
            "Epoch: [300][0/55]\tTime 0.721 (0.721)\tData 0.541 (0.541)\tLoss 0.0540 (0.0540)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.399999999999859e-13\n",
            "Epoch: [301][0/55]\tTime 0.748 (0.748)\tData 0.542 (0.542)\tLoss 0.2743 (0.2743)\tPrec@1 93.750 (93.750)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.399999999999859e-13\n",
            "Epoch: [302][0/55]\tTime 0.759 (0.759)\tData 0.580 (0.580)\tLoss 0.1884 (0.1884)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.399999999999859e-13\n",
            "Epoch: [303][0/55]\tTime 0.766 (0.766)\tData 0.581 (0.581)\tLoss 0.1019 (0.1019)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.399999999999859e-13\n",
            "Epoch: [304][0/55]\tTime 0.747 (0.747)\tData 0.569 (0.569)\tLoss 0.1284 (0.1284)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.399999999999859e-13\n",
            "Epoch: [305][0/55]\tTime 0.775 (0.775)\tData 0.559 (0.559)\tLoss 1.0391 (1.0391)\tPrec@1 90.625 (90.625)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.399999999999859e-13\n",
            "Epoch: [306][0/55]\tTime 0.770 (0.770)\tData 0.554 (0.554)\tLoss 0.1041 (0.1041)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.399999999999859e-13\n",
            "Epoch: [307][0/55]\tTime 0.753 (0.753)\tData 0.554 (0.554)\tLoss 0.4110 (0.4110)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.399999999999859e-13\n",
            "Epoch: [308][0/55]\tTime 0.757 (0.757)\tData 0.556 (0.556)\tLoss 0.1476 (0.1476)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.399999999999859e-13\n",
            "Epoch: [309][0/55]\tTime 0.734 (0.734)\tData 0.568 (0.568)\tLoss 0.0692 (0.0692)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.926752332334822e-13\n",
            "Change lr\n",
            "Epoch: [310][0/55]\tTime 0.761 (0.761)\tData 0.570 (0.570)\tLoss 0.0677 (0.0677)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.926752332334822e-13\n",
            "Epoch: [311][0/55]\tTime 0.706 (0.706)\tData 0.542 (0.542)\tLoss 0.1565 (0.1565)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.926752332334822e-13\n",
            "Epoch: [312][0/55]\tTime 0.777 (0.777)\tData 0.605 (0.605)\tLoss 0.0691 (0.0691)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.926752332334822e-13\n",
            "Epoch: [313][0/55]\tTime 0.785 (0.785)\tData 0.559 (0.559)\tLoss 0.1082 (0.1082)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.926752332334822e-13\n",
            "Epoch: [314][0/55]\tTime 0.717 (0.717)\tData 0.531 (0.531)\tLoss 0.1376 (0.1376)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.926752332334822e-13\n",
            "Epoch: [315][0/55]\tTime 0.758 (0.758)\tData 0.566 (0.566)\tLoss 0.1541 (0.1541)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.926752332334822e-13\n",
            "Epoch: [316][0/55]\tTime 0.737 (0.737)\tData 0.568 (0.568)\tLoss 0.0933 (0.0933)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.926752332334822e-13\n",
            "Epoch: [317][0/55]\tTime 0.748 (0.748)\tData 0.554 (0.554)\tLoss 0.4117 (0.4117)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.926752332334822e-13\n",
            "Epoch: [318][0/55]\tTime 0.737 (0.737)\tData 0.547 (0.547)\tLoss 0.1738 (0.1738)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.926752332334822e-13\n",
            "Epoch: [319][0/55]\tTime 0.792 (0.792)\tData 0.602 (0.602)\tLoss 0.1013 (0.1013)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.3384186273167984e-13\n",
            "Change lr\n",
            "Epoch: [320][0/55]\tTime 0.743 (0.743)\tData 0.558 (0.558)\tLoss 0.0536 (0.0536)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.3384186273167984e-13\n",
            "Epoch: [321][0/55]\tTime 0.771 (0.771)\tData 0.556 (0.556)\tLoss 0.0804 (0.0804)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.3384186273167984e-13\n",
            "Epoch: [322][0/55]\tTime 0.717 (0.717)\tData 0.540 (0.540)\tLoss 0.0718 (0.0718)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.3384186273167984e-13\n",
            "Epoch: [323][0/55]\tTime 0.740 (0.740)\tData 0.544 (0.544)\tLoss 0.1308 (0.1308)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.3384186273167984e-13\n",
            "Epoch: [324][0/55]\tTime 0.745 (0.745)\tData 0.586 (0.586)\tLoss 0.0359 (0.0359)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.3384186273167984e-13\n",
            "Epoch: [325][0/55]\tTime 0.752 (0.752)\tData 0.547 (0.547)\tLoss 0.1064 (0.1064)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.3384186273167984e-13\n",
            "Epoch: [326][0/55]\tTime 0.752 (0.752)\tData 0.565 (0.565)\tLoss 0.0940 (0.0940)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.3384186273167984e-13\n",
            "Epoch: [327][0/55]\tTime 0.752 (0.752)\tData 0.582 (0.582)\tLoss 0.0677 (0.0677)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.3384186273167984e-13\n",
            "Epoch: [328][0/55]\tTime 0.765 (0.765)\tData 0.579 (0.579)\tLoss 0.0731 (0.0731)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.3384186273167984e-13\n",
            "Epoch: [329][0/55]\tTime 0.713 (0.713)\tData 0.589 (0.589)\tLoss 0.0468 (0.0468)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.120655998656089e-14\n",
            "Change lr\n",
            "Epoch: [330][0/55]\tTime 0.784 (0.784)\tData 0.597 (0.597)\tLoss 0.1540 (0.1540)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.120655998656089e-14\n",
            "Epoch: [331][0/55]\tTime 0.746 (0.746)\tData 0.553 (0.553)\tLoss 0.0966 (0.0966)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.120655998656089e-14\n",
            "Epoch: [332][0/55]\tTime 0.743 (0.743)\tData 0.581 (0.581)\tLoss 0.1268 (0.1268)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.120655998656089e-14\n",
            "Epoch: [333][0/55]\tTime 0.750 (0.750)\tData 0.565 (0.565)\tLoss 0.1259 (0.1259)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.120655998656089e-14\n",
            "Epoch: [334][0/55]\tTime 0.714 (0.714)\tData 0.560 (0.560)\tLoss 0.0572 (0.0572)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.120655998656089e-14\n",
            "Epoch: [335][0/55]\tTime 0.772 (0.772)\tData 0.612 (0.612)\tLoss 1.3911 (1.3911)\tPrec@1 96.875 (96.875)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.120655998656089e-14\n",
            "Epoch: [336][0/55]\tTime 0.738 (0.738)\tData 0.545 (0.545)\tLoss 0.0730 (0.0730)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.120655998656089e-14\n",
            "Epoch: [337][0/55]\tTime 0.772 (0.772)\tData 0.553 (0.553)\tLoss 0.1856 (0.1856)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.120655998656089e-14\n",
            "Epoch: [338][0/55]\tTime 0.753 (0.753)\tData 0.546 (0.546)\tLoss 1.3056 (1.3056)\tPrec@1 93.750 (93.750)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 6.120655998656089e-14\n",
            "Epoch: [339][0/55]\tTime 0.723 (0.723)\tData 0.530 (0.530)\tLoss 0.1007 (0.1007)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.799006909294722e-14\n",
            "Change lr\n",
            "Epoch: [340][0/55]\tTime 0.756 (0.756)\tData 0.568 (0.568)\tLoss 0.5814 (0.5814)\tPrec@1 87.500 (87.500)\tPrec@5 93.750 (93.750)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.799006909294722e-14\n",
            "Epoch: [341][0/55]\tTime 0.774 (0.774)\tData 0.561 (0.561)\tLoss 0.0658 (0.0658)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.799006909294722e-14\n",
            "Epoch: [342][0/55]\tTime 0.751 (0.751)\tData 0.561 (0.561)\tLoss 0.1682 (0.1682)\tPrec@1 93.750 (93.750)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.799006909294722e-14\n",
            "Epoch: [343][0/55]\tTime 0.733 (0.733)\tData 0.565 (0.565)\tLoss 0.4733 (0.4733)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.799006909294722e-14\n",
            "Epoch: [344][0/55]\tTime 0.769 (0.769)\tData 0.564 (0.564)\tLoss 0.0975 (0.0975)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.799006909294722e-14\n",
            "Epoch: [345][0/55]\tTime 0.703 (0.703)\tData 0.550 (0.550)\tLoss 0.0856 (0.0856)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.799006909294722e-14\n",
            "Epoch: [346][0/55]\tTime 0.740 (0.740)\tData 0.555 (0.555)\tLoss 0.1012 (0.1012)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.799006909294722e-14\n",
            "Epoch: [347][0/55]\tTime 0.750 (0.750)\tData 0.554 (0.554)\tLoss 0.1787 (0.1787)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.799006909294722e-14\n",
            "Epoch: [348][0/55]\tTime 0.780 (0.780)\tData 0.562 (0.562)\tLoss 0.0551 (0.0551)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.799006909294722e-14\n",
            "Epoch: [349][0/55]\tTime 0.732 (0.732)\tData 0.532 (0.532)\tLoss 0.0730 (0.0730)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.2799999999999673e-14\n",
            "Change lr\n",
            "Epoch: [350][0/55]\tTime 0.696 (0.696)\tData 0.529 (0.529)\tLoss 0.1213 (0.1213)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.2799999999999673e-14\n",
            "Epoch: [351][0/55]\tTime 0.738 (0.738)\tData 0.563 (0.563)\tLoss 0.1063 (0.1063)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.2799999999999673e-14\n",
            "Epoch: [352][0/55]\tTime 0.746 (0.746)\tData 0.592 (0.592)\tLoss 0.1244 (0.1244)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.2799999999999673e-14\n",
            "Epoch: [353][0/55]\tTime 0.744 (0.744)\tData 0.563 (0.563)\tLoss 0.0246 (0.0246)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.2799999999999673e-14\n",
            "Epoch: [354][0/55]\tTime 0.766 (0.766)\tData 0.595 (0.595)\tLoss 0.6936 (0.6936)\tPrec@1 90.625 (90.625)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.2799999999999673e-14\n",
            "Epoch: [355][0/55]\tTime 0.745 (0.745)\tData 0.578 (0.578)\tLoss 0.0191 (0.0191)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.2799999999999673e-14\n",
            "Epoch: [356][0/55]\tTime 0.739 (0.739)\tData 0.554 (0.554)\tLoss 0.1312 (0.1312)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.2799999999999673e-14\n",
            "Epoch: [357][0/55]\tTime 0.779 (0.779)\tData 0.575 (0.575)\tLoss 0.1463 (0.1463)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.2799999999999673e-14\n",
            "Epoch: [358][0/55]\tTime 0.737 (0.737)\tData 0.581 (0.581)\tLoss 0.0266 (0.0266)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.2799999999999673e-14\n",
            "Epoch: [359][0/55]\tTime 0.764 (0.764)\tData 0.575 (0.575)\tLoss 0.0521 (0.0521)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.8535046646696236e-15\n",
            "Change lr\n",
            "Epoch: [360][0/55]\tTime 0.759 (0.759)\tData 0.583 (0.583)\tLoss 0.0466 (0.0466)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.8535046646696236e-15\n",
            "Epoch: [361][0/55]\tTime 0.752 (0.752)\tData 0.578 (0.578)\tLoss 0.1123 (0.1123)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.8535046646696236e-15\n",
            "Epoch: [362][0/55]\tTime 0.750 (0.750)\tData 0.558 (0.558)\tLoss 0.0777 (0.0777)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.8535046646696236e-15\n",
            "Epoch: [363][0/55]\tTime 0.718 (0.718)\tData 0.549 (0.549)\tLoss 0.1311 (0.1311)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.8535046646696236e-15\n",
            "Epoch: [364][0/55]\tTime 0.794 (0.794)\tData 0.582 (0.582)\tLoss 0.1576 (0.1576)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.8535046646696236e-15\n",
            "Epoch: [365][0/55]\tTime 0.800 (0.800)\tData 0.583 (0.583)\tLoss 0.0634 (0.0634)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.8535046646696236e-15\n",
            "Epoch: [366][0/55]\tTime 0.757 (0.757)\tData 0.551 (0.551)\tLoss 0.2588 (0.2588)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.8535046646696236e-15\n",
            "Epoch: [367][0/55]\tTime 0.739 (0.739)\tData 0.554 (0.554)\tLoss 0.0769 (0.0769)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.8535046646696236e-15\n",
            "Epoch: [368][0/55]\tTime 0.770 (0.770)\tData 0.551 (0.551)\tLoss 0.0737 (0.0737)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.8535046646696236e-15\n",
            "Epoch: [369][0/55]\tTime 0.806 (0.806)\tData 0.579 (0.579)\tLoss 0.1149 (0.1149)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.6768372546335868e-15\n",
            "Change lr\n",
            "Epoch: [370][0/55]\tTime 0.773 (0.773)\tData 0.588 (0.588)\tLoss 0.0723 (0.0723)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.6768372546335868e-15\n",
            "Epoch: [371][0/55]\tTime 0.714 (0.714)\tData 0.538 (0.538)\tLoss 0.1205 (0.1205)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.6768372546335868e-15\n",
            "Epoch: [372][0/55]\tTime 0.792 (0.792)\tData 0.594 (0.594)\tLoss 0.2428 (0.2428)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.6768372546335868e-15\n",
            "Epoch: [373][0/55]\tTime 0.785 (0.785)\tData 0.603 (0.603)\tLoss 0.1068 (0.1068)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.6768372546335868e-15\n",
            "Epoch: [374][0/55]\tTime 0.702 (0.702)\tData 0.553 (0.553)\tLoss 0.0982 (0.0982)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.6768372546335868e-15\n",
            "Epoch: [375][0/55]\tTime 0.787 (0.787)\tData 0.609 (0.609)\tLoss 0.0253 (0.0253)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.6768372546335868e-15\n",
            "Epoch: [376][0/55]\tTime 0.789 (0.789)\tData 0.610 (0.610)\tLoss 0.0512 (0.0512)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.6768372546335868e-15\n",
            "Epoch: [377][0/55]\tTime 0.711 (0.711)\tData 0.523 (0.523)\tLoss 0.0316 (0.0316)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.6768372546335868e-15\n",
            "Epoch: [378][0/55]\tTime 0.775 (0.775)\tData 0.562 (0.562)\tLoss 0.0950 (0.0950)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.6768372546335868e-15\n",
            "Epoch: [379][0/55]\tTime 0.715 (0.715)\tData 0.559 (0.559)\tLoss 0.0874 (0.0874)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.2241311997312132e-15\n",
            "Change lr\n",
            "Epoch: [380][0/55]\tTime 0.737 (0.737)\tData 0.553 (0.553)\tLoss 0.1545 (0.1545)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.2241311997312132e-15\n",
            "Epoch: [381][0/55]\tTime 0.764 (0.764)\tData 0.563 (0.563)\tLoss 0.1952 (0.1952)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.2241311997312132e-15\n",
            "Epoch: [382][0/55]\tTime 0.738 (0.738)\tData 0.543 (0.543)\tLoss 0.2591 (0.2591)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.2241311997312132e-15\n",
            "Epoch: [383][0/55]\tTime 0.739 (0.739)\tData 0.542 (0.542)\tLoss 0.0478 (0.0478)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.2241311997312132e-15\n",
            "Epoch: [384][0/55]\tTime 0.805 (0.805)\tData 0.629 (0.629)\tLoss 0.0656 (0.0656)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.2241311997312132e-15\n",
            "Epoch: [385][0/55]\tTime 0.751 (0.751)\tData 0.562 (0.562)\tLoss 0.1187 (0.1187)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.2241311997312132e-15\n",
            "Epoch: [386][0/55]\tTime 0.765 (0.765)\tData 0.557 (0.557)\tLoss 0.1583 (0.1583)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.2241311997312132e-15\n",
            "Epoch: [387][0/55]\tTime 0.727 (0.727)\tData 0.562 (0.562)\tLoss 0.1561 (0.1561)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.2241311997312132e-15\n",
            "Epoch: [388][0/55]\tTime 0.802 (0.802)\tData 0.645 (0.645)\tLoss 0.2005 (0.2005)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.2241311997312132e-15\n",
            "Epoch: [389][0/55]\tTime 0.725 (0.725)\tData 0.568 (0.568)\tLoss 0.0851 (0.0851)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.598013818589424e-16\n",
            "Change lr\n",
            "Epoch: [390][0/55]\tTime 0.749 (0.749)\tData 0.534 (0.534)\tLoss 0.2517 (0.2517)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.598013818589424e-16\n",
            "Epoch: [391][0/55]\tTime 0.770 (0.770)\tData 0.542 (0.542)\tLoss 0.0293 (0.0293)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.598013818589424e-16\n",
            "Epoch: [392][0/55]\tTime 0.769 (0.769)\tData 0.598 (0.598)\tLoss 0.1274 (0.1274)\tPrec@1 96.875 (96.875)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.598013818589424e-16\n",
            "Epoch: [393][0/55]\tTime 0.781 (0.781)\tData 0.581 (0.581)\tLoss 0.0197 (0.0197)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.598013818589424e-16\n",
            "Epoch: [394][0/55]\tTime 0.766 (0.766)\tData 0.565 (0.565)\tLoss 0.0163 (0.0163)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.598013818589424e-16\n",
            "Epoch: [395][0/55]\tTime 0.786 (0.786)\tData 0.581 (0.581)\tLoss 0.1211 (0.1211)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.598013818589424e-16\n",
            "Epoch: [396][0/55]\tTime 0.753 (0.753)\tData 0.559 (0.559)\tLoss 0.1634 (0.1634)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.598013818589424e-16\n",
            "Epoch: [397][0/55]\tTime 0.735 (0.735)\tData 0.543 (0.543)\tLoss 0.0859 (0.0859)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.598013818589424e-16\n",
            "Epoch: [398][0/55]\tTime 0.725 (0.725)\tData 0.551 (0.551)\tLoss 0.0683 (0.0683)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.598013818589424e-16\n",
            "Epoch: [399][0/55]\tTime 0.750 (0.750)\tData 0.557 (0.557)\tLoss 0.0491 (0.0491)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.559999999999925e-16\n",
            "Change lr\n",
            "Epoch: [400][0/55]\tTime 0.747 (0.747)\tData 0.556 (0.556)\tLoss 0.1919 (0.1919)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.559999999999925e-16\n",
            "Epoch: [401][0/55]\tTime 0.780 (0.780)\tData 0.580 (0.580)\tLoss 0.0939 (0.0939)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.559999999999925e-16\n",
            "Epoch: [402][0/55]\tTime 0.773 (0.773)\tData 0.549 (0.549)\tLoss 0.0455 (0.0455)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.559999999999925e-16\n",
            "Epoch: [403][0/55]\tTime 0.764 (0.764)\tData 0.526 (0.526)\tLoss 0.0759 (0.0759)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.559999999999925e-16\n",
            "Epoch: [404][0/55]\tTime 0.777 (0.777)\tData 0.566 (0.566)\tLoss 0.1143 (0.1143)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.559999999999925e-16\n",
            "Epoch: [405][0/55]\tTime 0.762 (0.762)\tData 0.558 (0.558)\tLoss 0.1307 (0.1307)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.559999999999925e-16\n",
            "Epoch: [406][0/55]\tTime 0.733 (0.733)\tData 0.561 (0.561)\tLoss 0.0628 (0.0628)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.559999999999925e-16\n",
            "Epoch: [407][0/55]\tTime 0.744 (0.744)\tData 0.558 (0.558)\tLoss 0.1299 (0.1299)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.559999999999925e-16\n",
            "Epoch: [408][0/55]\tTime 0.778 (0.778)\tData 0.551 (0.551)\tLoss 0.1941 (0.1941)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.559999999999925e-16\n",
            "Epoch: [409][0/55]\tTime 0.764 (0.764)\tData 0.559 (0.559)\tLoss 0.0379 (0.0379)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.1707009329339203e-16\n",
            "Change lr\n",
            "Epoch: [410][0/55]\tTime 0.806 (0.806)\tData 0.597 (0.597)\tLoss 0.1161 (0.1161)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.1707009329339203e-16\n",
            "Epoch: [411][0/55]\tTime 0.808 (0.808)\tData 0.569 (0.569)\tLoss 0.0653 (0.0653)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.1707009329339203e-16\n",
            "Epoch: [412][0/55]\tTime 0.792 (0.792)\tData 0.569 (0.569)\tLoss 0.3345 (0.3345)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.1707009329339203e-16\n",
            "Epoch: [413][0/55]\tTime 0.720 (0.720)\tData 0.539 (0.539)\tLoss 0.0844 (0.0844)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.1707009329339203e-16\n",
            "Epoch: [414][0/55]\tTime 0.780 (0.780)\tData 0.570 (0.570)\tLoss 0.1218 (0.1218)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.1707009329339203e-16\n",
            "Epoch: [415][0/55]\tTime 0.775 (0.775)\tData 0.605 (0.605)\tLoss 0.2632 (0.2632)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.1707009329339203e-16\n",
            "Epoch: [416][0/55]\tTime 0.771 (0.771)\tData 0.552 (0.552)\tLoss 0.1023 (0.1023)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.1707009329339203e-16\n",
            "Epoch: [417][0/55]\tTime 0.786 (0.786)\tData 0.559 (0.559)\tLoss 0.1302 (0.1302)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.1707009329339203e-16\n",
            "Epoch: [418][0/55]\tTime 0.798 (0.798)\tData 0.564 (0.564)\tLoss 0.0490 (0.0490)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.1707009329339203e-16\n",
            "Epoch: [419][0/55]\tTime 0.735 (0.735)\tData 0.551 (0.551)\tLoss 0.5883 (0.5883)\tPrec@1 96.875 (96.875)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.353674509267155e-17\n",
            "Change lr\n",
            "Epoch: [420][0/55]\tTime 0.760 (0.760)\tData 0.573 (0.573)\tLoss 0.1968 (0.1968)\tPrec@1 93.750 (93.750)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.353674509267155e-17\n",
            "Epoch: [421][0/55]\tTime 0.719 (0.719)\tData 0.543 (0.543)\tLoss 0.0116 (0.0116)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.353674509267155e-17\n",
            "Epoch: [422][0/55]\tTime 0.751 (0.751)\tData 0.566 (0.566)\tLoss 0.1156 (0.1156)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.353674509267155e-17\n",
            "Epoch: [423][0/55]\tTime 0.772 (0.772)\tData 0.630 (0.630)\tLoss 0.1336 (0.1336)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.353674509267155e-17\n",
            "Epoch: [424][0/55]\tTime 0.766 (0.766)\tData 0.570 (0.570)\tLoss 0.1764 (0.1764)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.353674509267155e-17\n",
            "Epoch: [425][0/55]\tTime 0.734 (0.734)\tData 0.575 (0.575)\tLoss 0.1634 (0.1634)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.353674509267155e-17\n",
            "Epoch: [426][0/55]\tTime 0.744 (0.744)\tData 0.571 (0.571)\tLoss 0.6399 (0.6399)\tPrec@1 90.625 (90.625)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.353674509267155e-17\n",
            "Epoch: [427][0/55]\tTime 0.754 (0.754)\tData 0.576 (0.576)\tLoss 0.1739 (0.1739)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.353674509267155e-17\n",
            "Epoch: [428][0/55]\tTime 0.716 (0.716)\tData 0.527 (0.527)\tLoss 0.0918 (0.0918)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.353674509267155e-17\n",
            "Epoch: [429][0/55]\tTime 0.747 (0.747)\tData 0.551 (0.551)\tLoss 0.0940 (0.0940)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.4482623994624178e-17\n",
            "Change lr\n",
            "Epoch: [430][0/55]\tTime 0.772 (0.772)\tData 0.571 (0.571)\tLoss 0.0241 (0.0241)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.4482623994624178e-17\n",
            "Epoch: [431][0/55]\tTime 0.726 (0.726)\tData 0.544 (0.544)\tLoss 0.0643 (0.0643)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.4482623994624178e-17\n",
            "Epoch: [432][0/55]\tTime 0.771 (0.771)\tData 0.563 (0.563)\tLoss 0.0472 (0.0472)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.4482623994624178e-17\n",
            "Epoch: [433][0/55]\tTime 0.774 (0.774)\tData 0.545 (0.545)\tLoss 0.1816 (0.1816)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.4482623994624178e-17\n",
            "Epoch: [434][0/55]\tTime 0.760 (0.760)\tData 0.548 (0.548)\tLoss 0.0670 (0.0670)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.4482623994624178e-17\n",
            "Epoch: [435][0/55]\tTime 0.748 (0.748)\tData 0.585 (0.585)\tLoss 0.1491 (0.1491)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.4482623994624178e-17\n",
            "Epoch: [436][0/55]\tTime 0.778 (0.778)\tData 0.586 (0.586)\tLoss 0.0582 (0.0582)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.4482623994624178e-17\n",
            "Epoch: [437][0/55]\tTime 0.776 (0.776)\tData 0.576 (0.576)\tLoss 0.0824 (0.0824)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.4482623994624178e-17\n",
            "Epoch: [438][0/55]\tTime 0.773 (0.773)\tData 0.545 (0.545)\tLoss 0.1154 (0.1154)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.4482623994624178e-17\n",
            "Epoch: [439][0/55]\tTime 0.761 (0.761)\tData 0.548 (0.548)\tLoss 0.0986 (0.0986)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.1196027637178808e-17\n",
            "Change lr\n",
            "Epoch: [440][0/55]\tTime 0.736 (0.736)\tData 0.548 (0.548)\tLoss 0.0766 (0.0766)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.1196027637178808e-17\n",
            "Epoch: [441][0/55]\tTime 0.756 (0.756)\tData 0.571 (0.571)\tLoss 0.1508 (0.1508)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.1196027637178808e-17\n",
            "Epoch: [442][0/55]\tTime 0.743 (0.743)\tData 0.536 (0.536)\tLoss 0.1128 (0.1128)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.1196027637178808e-17\n",
            "Epoch: [443][0/55]\tTime 0.803 (0.803)\tData 0.570 (0.570)\tLoss 0.0235 (0.0235)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.1196027637178808e-17\n",
            "Epoch: [444][0/55]\tTime 0.774 (0.774)\tData 0.601 (0.601)\tLoss 0.0786 (0.0786)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.1196027637178808e-17\n",
            "Epoch: [445][0/55]\tTime 0.789 (0.789)\tData 0.570 (0.570)\tLoss 0.1552 (0.1552)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.1196027637178808e-17\n",
            "Epoch: [446][0/55]\tTime 0.756 (0.756)\tData 0.575 (0.575)\tLoss 0.1918 (0.1918)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.1196027637178808e-17\n",
            "Epoch: [447][0/55]\tTime 0.774 (0.774)\tData 0.555 (0.555)\tLoss 0.1779 (0.1779)\tPrec@1 96.875 (96.875)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.1196027637178808e-17\n",
            "Epoch: [448][0/55]\tTime 0.746 (0.746)\tData 0.549 (0.549)\tLoss 0.5300 (0.5300)\tPrec@1 90.625 (90.625)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.1196027637178808e-17\n",
            "Epoch: [449][0/55]\tTime 0.727 (0.727)\tData 0.532 (0.532)\tLoss 0.1702 (0.1702)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.119999999999832e-18\n",
            "Change lr\n",
            "Epoch: [450][0/55]\tTime 0.786 (0.786)\tData 0.582 (0.582)\tLoss 0.8758 (0.8758)\tPrec@1 87.500 (87.500)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.119999999999832e-18\n",
            "Epoch: [451][0/55]\tTime 0.727 (0.727)\tData 0.560 (0.560)\tLoss 0.1106 (0.1106)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.119999999999832e-18\n",
            "Epoch: [452][0/55]\tTime 0.762 (0.762)\tData 0.550 (0.550)\tLoss 0.1014 (0.1014)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.119999999999832e-18\n",
            "Epoch: [453][0/55]\tTime 0.775 (0.775)\tData 0.560 (0.560)\tLoss 0.0264 (0.0264)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.119999999999832e-18\n",
            "Epoch: [454][0/55]\tTime 0.737 (0.737)\tData 0.552 (0.552)\tLoss 0.1340 (0.1340)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.119999999999832e-18\n",
            "Epoch: [455][0/55]\tTime 0.784 (0.784)\tData 0.595 (0.595)\tLoss 0.1446 (0.1446)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.119999999999832e-18\n",
            "Epoch: [456][0/55]\tTime 0.783 (0.783)\tData 0.561 (0.561)\tLoss 0.3008 (0.3008)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.119999999999832e-18\n",
            "Epoch: [457][0/55]\tTime 0.720 (0.720)\tData 0.560 (0.560)\tLoss 0.2303 (0.2303)\tPrec@1 93.750 (93.750)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.119999999999832e-18\n",
            "Epoch: [458][0/55]\tTime 0.730 (0.730)\tData 0.573 (0.573)\tLoss 0.0666 (0.0666)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 5.119999999999832e-18\n",
            "Epoch: [459][0/55]\tTime 0.772 (0.772)\tData 0.590 (0.590)\tLoss 0.1260 (0.1260)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.3414018658678324e-18\n",
            "Change lr\n",
            "Epoch: [460][0/55]\tTime 0.786 (0.786)\tData 0.575 (0.575)\tLoss 0.1451 (0.1451)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.3414018658678324e-18\n",
            "Epoch: [461][0/55]\tTime 0.770 (0.770)\tData 0.580 (0.580)\tLoss 0.1052 (0.1052)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.3414018658678324e-18\n",
            "Epoch: [462][0/55]\tTime 0.746 (0.746)\tData 0.555 (0.555)\tLoss 0.3643 (0.3643)\tPrec@1 93.750 (93.750)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.3414018658678324e-18\n",
            "Epoch: [463][0/55]\tTime 0.754 (0.754)\tData 0.567 (0.567)\tLoss 0.0558 (0.0558)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.3414018658678324e-18\n",
            "Epoch: [464][0/55]\tTime 0.784 (0.784)\tData 0.560 (0.560)\tLoss 0.0882 (0.0882)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.3414018658678324e-18\n",
            "Epoch: [465][0/55]\tTime 0.705 (0.705)\tData 0.567 (0.567)\tLoss 0.0426 (0.0426)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.3414018658678324e-18\n",
            "Epoch: [466][0/55]\tTime 0.767 (0.767)\tData 0.566 (0.566)\tLoss 0.0325 (0.0325)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.3414018658678324e-18\n",
            "Epoch: [467][0/55]\tTime 0.798 (0.798)\tData 0.583 (0.583)\tLoss 0.0963 (0.0963)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.3414018658678324e-18\n",
            "Epoch: [468][0/55]\tTime 0.692 (0.692)\tData 0.538 (0.538)\tLoss 0.1327 (0.1327)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.3414018658678324e-18\n",
            "Epoch: [469][0/55]\tTime 0.764 (0.764)\tData 0.562 (0.562)\tLoss 0.0507 (0.0507)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.070734901853427e-18\n",
            "Change lr\n",
            "Epoch: [470][0/55]\tTime 0.785 (0.785)\tData 0.555 (0.555)\tLoss 0.0638 (0.0638)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.070734901853427e-18\n",
            "Epoch: [471][0/55]\tTime 0.747 (0.747)\tData 0.569 (0.569)\tLoss 0.8013 (0.8013)\tPrec@1 96.875 (96.875)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.070734901853427e-18\n",
            "Epoch: [472][0/55]\tTime 0.792 (0.792)\tData 0.578 (0.578)\tLoss 0.1070 (0.1070)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.070734901853427e-18\n",
            "Epoch: [473][0/55]\tTime 0.902 (0.902)\tData 0.706 (0.706)\tLoss 0.0459 (0.0459)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.070734901853427e-18\n",
            "Epoch: [474][0/55]\tTime 0.767 (0.767)\tData 0.565 (0.565)\tLoss 0.0334 (0.0334)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.070734901853427e-18\n",
            "Epoch: [475][0/55]\tTime 0.761 (0.761)\tData 0.581 (0.581)\tLoss 0.1980 (0.1980)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.070734901853427e-18\n",
            "Epoch: [476][0/55]\tTime 0.733 (0.733)\tData 0.576 (0.576)\tLoss 0.0275 (0.0275)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.070734901853427e-18\n",
            "Epoch: [477][0/55]\tTime 0.767 (0.767)\tData 0.567 (0.567)\tLoss 0.0990 (0.0990)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.070734901853427e-18\n",
            "Epoch: [478][0/55]\tTime 0.745 (0.745)\tData 0.559 (0.559)\tLoss 0.0693 (0.0693)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.070734901853427e-18\n",
            "Epoch: [479][0/55]\tTime 0.775 (0.775)\tData 0.552 (0.552)\tLoss 0.1458 (0.1458)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 4.896524798924817e-19\n",
            "Change lr\n",
            "Epoch: [480][0/55]\tTime 0.801 (0.801)\tData 0.567 (0.567)\tLoss 0.0237 (0.0237)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 4.896524798924817e-19\n",
            "Epoch: [481][0/55]\tTime 0.765 (0.765)\tData 0.584 (0.584)\tLoss 0.0645 (0.0645)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 4.896524798924817e-19\n",
            "Epoch: [482][0/55]\tTime 0.788 (0.788)\tData 0.565 (0.565)\tLoss 0.0543 (0.0543)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 4.896524798924817e-19\n",
            "Epoch: [483][0/55]\tTime 0.789 (0.789)\tData 0.572 (0.572)\tLoss 0.1901 (0.1901)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 4.896524798924817e-19\n",
            "Epoch: [484][0/55]\tTime 0.728 (0.728)\tData 0.561 (0.561)\tLoss 0.2120 (0.2120)\tPrec@1 93.750 (93.750)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 4.896524798924817e-19\n",
            "Epoch: [485][0/55]\tTime 0.767 (0.767)\tData 0.559 (0.559)\tLoss 0.0292 (0.0292)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 4.896524798924817e-19\n",
            "Epoch: [486][0/55]\tTime 0.757 (0.757)\tData 0.572 (0.572)\tLoss 0.2146 (0.2146)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 4.896524798924817e-19\n",
            "Epoch: [487][0/55]\tTime 0.775 (0.775)\tData 0.580 (0.580)\tLoss 0.0288 (0.0288)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 4.896524798924817e-19\n",
            "Epoch: [488][0/55]\tTime 0.737 (0.737)\tData 0.557 (0.557)\tLoss 0.2163 (0.2163)\tPrec@1 87.500 (87.500)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 4.896524798924817e-19\n",
            "Epoch: [489][0/55]\tTime 0.762 (0.762)\tData 0.562 (0.562)\tLoss 0.0753 (0.0753)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.239205527435753e-19\n",
            "Change lr\n",
            "Epoch: [490][0/55]\tTime 0.762 (0.762)\tData 0.550 (0.550)\tLoss 0.1838 (0.1838)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.239205527435753e-19\n",
            "Epoch: [491][0/55]\tTime 0.757 (0.757)\tData 0.575 (0.575)\tLoss 0.1783 (0.1783)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.239205527435753e-19\n",
            "Epoch: [492][0/55]\tTime 0.714 (0.714)\tData 0.548 (0.548)\tLoss 0.2304 (0.2304)\tPrec@1 90.625 (90.625)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.239205527435753e-19\n",
            "Epoch: [493][0/55]\tTime 0.728 (0.728)\tData 0.538 (0.538)\tLoss 0.1713 (0.1713)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.239205527435753e-19\n",
            "Epoch: [494][0/55]\tTime 0.756 (0.756)\tData 0.554 (0.554)\tLoss 0.4919 (0.4919)\tPrec@1 96.875 (96.875)\tPrec@5 96.875 (96.875)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.239205527435753e-19\n",
            "Epoch: [495][0/55]\tTime 0.749 (0.749)\tData 0.550 (0.550)\tLoss 0.1181 (0.1181)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.239205527435753e-19\n",
            "Epoch: [496][0/55]\tTime 0.755 (0.755)\tData 0.551 (0.551)\tLoss 0.1444 (0.1444)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.239205527435753e-19\n",
            "Epoch: [497][0/55]\tTime 0.765 (0.765)\tData 0.584 (0.584)\tLoss 0.0291 (0.0291)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.239205527435753e-19\n",
            "Epoch: [498][0/55]\tTime 0.707 (0.707)\tData 0.547 (0.547)\tLoss 0.1023 (0.1023)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 2.239205527435753e-19\n",
            "Epoch: [499][0/55]\tTime 0.705 (0.705)\tData 0.522 (0.522)\tLoss 0.0492 (0.0492)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.0239999999999625e-19\n",
            "Change lr\n",
            "Epoch: [500][0/55]\tTime 0.751 (0.751)\tData 0.564 (0.564)\tLoss 0.0603 (0.0603)\tPrec@1 100.000 (100.000)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n",
            "lr: 1.0239999999999625e-19\n",
            "Epoch: [501][0/55]\tTime 0.788 (0.788)\tData 0.586 (0.586)\tLoss 0.1265 (0.1265)\tPrec@1 96.875 (96.875)\tPrec@5 100.000 (100.000)\n",
            "\n",
            "Test set: Average loss: 7.789432116365626, Accuracy: (25.91093118181113)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_qW6dGMqw_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "single_loss = []\n",
        "single_acc = []\n",
        "\n",
        "for item in loss_list:\n",
        "  single_loss.append(item[0])\n",
        "  single_acc.append(item[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZNc_kiKrofv",
        "colab_type": "code",
        "outputId": "788eab12-7f78-4cc5-b3ca-be8dec3cb442",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "fig, ((ax1, ax2)) = plt.subplots(1, 2)\n",
        "ax1.plot(np.arange(len(single_acc)), np.array(single_acc))\n",
        "ax2.plot(np.arange(len(single_loss)), single_loss, 'tab:orange')\n",
        "\n",
        "ax1.set_title('Accuracy (%)')\n",
        "ax2.set_title('Loss')\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXwV5dXA8d/JHrYESNiXgEEUkDUCKqKCu6i0Wqu2Qi2Vth9r3bpgN+32Vq2vVtvaV+uGLS6t2mrdFVG0KgqIiKCCLAKChD3sWc77x8xNbm7uTe4+uZPz/XzyuXNne84kk3Of+8wzz4iqYowxxl+yvA7AGGNM8llyN8YYH7LkbowxPmTJ3RhjfMiSuzHG+JAld2OM8SFL7gYAEckXkeUi0jOObYeLyJupiMsYEx9L7lEQkVdFZIeI5HsdSwrNBOar6iYAEblYRDaJyFoROSmwkogcJiJvikh2YJ6qLgV2isjZ6Q/btGbu+XOy13G0RZbcWyAiZcDxgALnpLnsnDQW9x3gb0Hl3giMBr4H/DFovTuAq1W1NmT7OcC30xCnMSYKltxbNg14G3gAmB68QET6isgTIlIpIttE5E9Byy4TkRUiUuU2d4x256uIlAet94CI/MadPlFENojIj0VkM3C/iHQWkafdMna4032Ctu8iIveLyOfu8n+785cF16RFJFdEtorIqNADFJF+wEBggTurK7DRrcW/7C5DRM535y8I3QfwKjDZ599uTBK4TYB/cM/Zz93pfHdZiXuO7xSR7SLyuohkuct+LCIb3f+pj0VksrdH0rpZcm/ZNJxa6RzgNBHpDuA2SzwNrAPKgN7AI+6yrwA3uNt2wqnxb4uyvB5AF6A/TlNJFnC/+74fsB/4U9D6fwPaAUOBbsBt7vwHga8HrXcmsElV3wtT5lHAalWtcd9XAl3dD5FTgA9FpCPwM+C6cEGr6kagGhgc5XGatuunwHhgJDACGItzbgFcC2wASoHuwE8AFZHBON8ij1bVjsBpwNr0hp1hVNV+IvwAE3ASVon7/iOcJgmAY3CSYE6Y7V4AroywTwXKg94/APzGnT4ROAQUNBPTSGCHO90TqAM6h1mvF1AFdHLfPwb8KMI+vwa8HTJvMs43ltfcMm8FZgAnAfPcYxwWss1GYKLXfzf7aT0/OAn45JB5nwJnBr0/DVjrTv8KeDL4f8SdXw5sAU4Gcr0+rkz4sZp786YDL6rqVvf9QzQ0zfQF1mlDbTdYX5wTOB6Vqnog8EZE2onIXSKyTkR2A/OBYvebQ19gu6ruCN2Jqn4O/Bc4T0SKgTNwvn2EswPoGLL9XFUdr6on4HwgVeB8ED0IfAP4NXBPyH46AjtjPF7T9vTC+cYbsM6dB/B7YBXwooisFpFZAKq6CrgK5xvxFhF5RER6YSKy5B6BiBQCFwAniMhmtw38amCEiIwA1gP9Ilz0XA8cFmHX+3CaUQJ6hCwPHabzWpymjnGq2gmYGAjRLaeLm7zDmY3TNPMV4C11mk7CWQoMCHcsIiI4zUDfB0qAbFVdB7wLDA9arzeQB3wcoQxjAj7HaWYM6OfOQ1WrVPVaVR2I05x5TaBtXVUfUtUJ7rYK3JTesDOLJffIpgK1wBCcZomRwJHA6zht6e8Am4AbRaS9iBSIyHHutvcAPxCRMeIoF5HAybwEuFhEskXkdOCEFuLoiNPOvlNEugDXBxaoc8HzOeBO98JrrohMDNr23zg9Xq7EqXGHpaobcGpLY8Ms/hawWFWX4Fw3KBSRITjNM6uD1jsBeEVVD7ZwPKbtyXX/PwpEpAB4GPiZiJSKSAnwC+DvACIyxf1/EWAXzv9gnYgMFpFJ7oXXAzj/E3XeHE5msOQe2XTgflX9TFU3B35warFfw6k5n43TFvgZzkWgrwKo6j+B3+I041ThJNku7n6vdLfb6e7n3y3E8QegENiK0wb+fMjyS3CuC3yE0yZ5VWCBqu4HHgcGAE+0UM5d7r7quf94VwI/d/dXg3NR6xXg/4Arglb/mjvPmFDP4iTjwE8BsBDnG+MHwGLgN+66g3B6aO0B3gLuVNV5QD5O99ytwGaczgNhL+4bh7gXK4xPicgvgMNV9estrJcPvAdMdr8RxFLGcOAuVT0m/kiNMclkyd3H3Gac94BLVHW+1/EYY9LHmmV8SkQuw7ng+pwldmPaHqu5G2OMD1nN3RhjfCidA1NFVFJSomVlZV6HYXxq0aJFW1W11Iuy7dw2qdTcud0qkntZWRkLFy70OgzjUyKyruW1UsPObZNKzZ3b1ixjjDE+ZMndGGN8yJK7Mcb4kCV3Y4zxIUvuxhjjQy0mdxG5T0S2iMiyoHldROQlEVnpvnZ254uI3CEiq0RkaeDRcsYYY9Irmpr7A8DpIfNmAXNVdRAw130PzgMhBrk/M4G/JCdMY4wxsWixn7uqzheRspDZ5+I8Eg6cB0K8CvzYnf+gOmMavC0ixSLSM9ZRBpPt8UUb+LRyD7sPVDOmf2dWbKqie6cCzjqqJ3+at5LvnlhO7+JC5n9SyQsfbmb9jv3kZWdx+rAevPXpNvJyhD6d23HWUT1ZtG4Hr31SSVFhLpt2HeDnU47k7vmr6VVcyN6DNQzq3oFB3Try6LvrmTFhAPe8sZrLTypn2cbd/Ou9DRQV5rJxp/OgpV5FBVSUdWHphp3UqTK6X2c27TrAqH7FPL9sM907FdCncyGlHfJ5b/1OHnn3M2ZfOpZbXvyYYw8r4cKj+/LEexvp3C6P99fvZMn6nWzcuZ81W/fSr0s7vjSqN88t28TzV04kK0sAWLdtL08v3cQJh5dSVJjLwnXbKczNprpWmTiolKJ2uQDM+3gLyzbsYl91LXsP1vDOmu1MPrIbZ4/oxeVzFvPrqcP49t8W8fcZ4/j4iyqmjuzNv5ds5K/zVzOybzEHa+o4ZUh3du6v5s1VW5l2TBmLP9vBceUlHNW7iJNvfY0pw3tyz+trOH9MHzbs2Mf9l47l9ZWV9OvSjn5d2jH1zjd5f/1OBnfvSHVdHZceN4DSDvms27aXHkUFnDuyN4vW7eAH/3yf2ZeOpV/XdhHPgVZl/0547JtQcSm07wZdBkIHT+6xMj4W1dgybnJ/WlWHue93qmqxOy04z/QsFpGngRtV9Q132Vzgx6ra5C4OEZmJU7unX79+Y9atS919JmWzngk7PydLqKlTOuTnsOyXp0VcL1ElHfLZuif5z7D4/qRy7nhlVYvr3Xz+cC6o6As0/l3kZgvVtQ1//xMHl/LApWObrJfMWABuOHsIN/xneZP5z115PGfc/joAD102jov/uqDZ/ay98axGca698ayw64nIIlWtiCq4JKuoqNAmNzFtXwN3jGw876JHofxkeOZqGPl16DcufUGajNXcuZ3wHaqqqiIS8+hjqno3cDc4/wCJxhGPmjqn2D0Hwz0GNXlSkdgBlm/aHdV6O/YeCjs/OLEDfL5zf9yxbKmK/hjDJXaAVz7aUj89d8WWsOsES9WHccp1Lms67+GvNkwvfhBu2JW2cIw/xdtb5gsR6Qngvgb+EzfiPLQ5oI87r9XbsGOf1yHE7OUoEiDAss9389onlS2u98kXe/jkiypWV+6JOZZd+6tj3ibU719oePzqvW+sSXh/rZYIzHjJ6yiMz8Wb3J/CeQwd7uuTQfOnub1mxgO7vG5vj9aEm+Z5HULK/Of9z5l+3zss/mxHi+ueett8Jv3vazGXkepvP77TdyxMuS3y8lUvpy8W40stNsuIyMM4F09LRGQDzgOabwT+ISIzgHXABe7qzwJn4jxseR9waQpiNnH68p1vpmzfr6/cmrJ9+1bFN52fgN/2hGr3G+SujPjCa1qxaHrLXBRh0eQw6ypweaJBJcsnX1TRr0uG9KAwGUFEioF7gGGAAt9U1beSsvPrNsBrNzX8jJne8jbGRODbO1SrDlRz6m3zufrRJV6HYtIkGe3+UbgdeF5VjwBGACuStuesbBj3HWd64IlJ261pm3yb3A/W1AGwYM12jyMx6XKgujal+xeRImAicC+Aqh5S1Z1JLaSws/O6ZA7safkiuDGR+Da5Z4lz005NbZ3HkZh06dYxP9VFDAAqgftF5D0RuUdE2oeuJCIzRWShiCysrIwxQbvnLQD7W74AbkwkPk7uzuvuA9aLo62Q4MSYGjnAaOAvqjoK2EvD0Bv1VPVuVa1Q1YrS0gTuPFWrmJj4+Ta5Cyn/RzetyOKfn5KOYjYAG1Q1cOvsYzjJPkU8ubfP+IR/k7tvj6z1OGdEr6Ts548XjUpo+15FBXRpn5eUWJqjqpuB9SIy2J01GQh/u21SCrSau4mfb1NgVuq/osdscPeOcS1rrbLC/IrvumRMzPsZ2qtTQnGM7Fec0PYxugKYIyJLgZHA/6SspCjGfTImEt8m90SdMawHv/3SMO7/xtEcXda50bKBJe15+ZqJzDrjCE4Z0j3s9uH61//nigmM6d+5yfzjB5Xw7JXHc/P5w+vn9SwqCLvfH542uH76xi8fVT999yVjuOUrIxrHWdrkWl8Tt184kjdnTeK5K4/n7zOaH6zqocvGcd7oPvXvs8Jk99OG9qifLsiNfHpNOqJbUJwd6NGp6fE+MnM8/501ibevm8xrPzwx7H5KOuTxv18ZGXZZKqjqErc9fbiqTlXV1F31tJq7SUCbS+5H9Iiuhtyvazu+Nq4/Jx3RjSN7Nq5ZPvrtYyjv1pHvnHAYt311JL2LC5tsf9LgxhfSJpSXkJeTxVG9i5qsO7pfZ7KzpH7kRqBJmQBd2udx+Unl9e8vHNuPBy49moGl7TlxcDfOH9On0foj+4av0R4/qKR+Wd8u7ehVXMiRPTsxYVBJ2PUDjj2shOvOPKJhRgsVy7OOatxs8/1JDbEHPvw6Fjj30Z1xVA9CjR/Yld7FhfQoKqB/1/AfVEN7FVGYl918IBnLau4mfm0uuR/Vu4irTh7UaN6E8qZJLfiCbOi349KgLncd8nP476xJTbcPaRYKvA03xHK4WnptXdP1wjU1nTi4G69ceyJ5OU3/lNkRmqYGlrRco48kN6uhnHA1d4Dybh3Czp8wqLS+xt7JTepTR/YG4m+BiBCCP+SF/z0aEw3fJvcRv3wx7HwRGNarce15aK9OPHTZuCbrBWhQDeqRmeOjKj80iQcSc2Dul0b15s1Zk7jrkjF89ei+hKoLk+2y3b/Wy9dM5OVrJrYYQ3aEzPeTs46Mqk74jWPLmjQ7FbXL5bShzrzC3Gweumwc7//i1EbrPDpzPI/OHN/o9wbO7zTwexnRt5j7vlHBz6YcCYT/0Av18jUn0CvkgzDSMfqC9QowCfDt2ROu5lu/LDSRiNPkECw4ZwSvPrxP02aVcLqFtCFn1dfcndeRfYvpVVzIaUN7hO2fHTa5u+uVd+tIebeWm5ci1azzcxqaMZpLjTlZwslHdmsy/5iBXZ39u7+3wNObArp2yGecu04kIjDpiO71sUTzYVPerQOHhXwrSEPfdu/s2+Z1BCaD+Ta5R9K5fR7tQtpoiwubdqNr1CwTND/aXjjdOxWw9sazePgyp6bfrWNBffkAnQqbH7OtpEPTuy17hWnbj3Uf9fG5TUsFuZHbqzu3z6NDfm6T+YHPzZYSa2j5hbnZ9U1ahbmNj79zu8Z/g/wwzUwApSH7TMNdqd65p8nYfMZELeEnMWWK4weVMLpfZ7574mHk52Txw9MG1z8c4lvHDwDgxasn8osnl/H26u2Nm2XcZDahvCRiMnz8u8eSkyV0KMjhxuc+4sujnLbk8QO7cPN5w5kyoicA3zupnB6dCjh3RO+w+3nthyeyunIvRw/owrtrtvP5Lud5q3dcNCrstYFQc741jpIO+Sxat4Pzx/RhSM+OfLS5irNH9OKpJZ/XXzT9/VdGcNryL5pcuL3tqyO4+tH3AZg5cSDZIlx6XBkTD2+4QBz4sAv+HT0yczxFhY0/CK455XDKuran6kA1RYW5DOtdxPVnD6WifxfGD+zSaN3LTyrn9rkrATiuvCt/vjj8vUG/mjqMBWu2s9F9atTPzhrS4u8kY518g9cRmAzWZpL730K6+X2log+/f+FjSjrkk+s2Zh/evSPjB3Z1knujtZ10Fq5HR0BwF8e/Tmt4pKGIcEFQm3peThYXj+sXcT/9u7av7xlyyTFl3PT8R/x8ypCobxg6zv0AGOz2Cjp9WE9OH+Z8sFx9yuH16xUV5nJeSO8agAnlThIv6ZBX/3u5/uyhjdYJtI8Hf4sZH6YZpiA3u8mxts/PafT7CAi+IDznW5Gva3TIz+G80b2545VVXHXyIB/3lAGKmv6ejIlWm0nuoQLNLsUh7cUaplraLs/5NRXkpDeRzJgwgCyBacf0T1uZoRdBwwlcD/CqtTsQYWu8US0ppj0JD55rbe4mIW02uZd2zOeX5wzl5JDeIA2Jo2HeNaccTnFhLueOTM7t9tHKy8ni2yccltYyG0ROnIEPwEgXbFOtrv6bgyfFp14nt8nuuR/BuG97G4vJWG02uQNMP7asyTytr5U2ZI72+TlcMXlQk3XbqoYLql6X79PsbnemmiTwZW+ZOQvWxb2tepy4Wou87Mi/gECNOcejqrPX5afcoT1eR2B8wJc195/+a1mj9zefNzzCmk0F2px9mjZaVNohnysmlTN1VPjePADTjilj4879fPfE8ojrxOPOr41u9v6EgO+eWE7VgRqmHVOW1PJbjeoDXkdgfMCXyT3UqUPDD+4VTluvuYsI1546uNl1CvOy+dW5w5Je9plH9YxqvQ75OSkpv9Wo3u91BMYHfNksEyqWC38NnWXaaHY33hvgDi3RO/bhk40JaBPJPZa22fpufpbbjVdy8qD0CChqeh+CMdFqE80yMfWHDjTLtNlWd9MqVH7k/BgTpzZRc49l5MBwt9YbY0ymaRPJPZZmma+P60/v4sK037BkTFj2qD0TJ98n97U3nhXTxdF+Xdvx31mT6FkU2wiMxqTEx895HYHJUL5P7sZktINVXkdgMpQld2NaNWuWMfHxdXIPPHzZmIy1fbXXEZgM5evs16mg6VOEjEmEiKwFqoBaoEZVK5rfIkHZdg6b+CRUcxeRq0XkQxFZJiIPi0iBiAwQkQUiskpEHhWRps+wS5N9h2q8Ktr420mqOjLliR2g37EpL8L4U9zJXUR6A98HKlR1GJANXAjcBNymquXADmBGMgI1pk350l1eR2AyXKJt7jlAoYjkAO2ATcAk4DF3+WxgaoJlxM3GhzEpoMCLIrJIRGamrJQ691vnU1ekrAjjb3End1XdCNwCfIaT1HcBi4CdqhpoD9kAhB07VkRmishCEVlYWVkZbxjN8utw38ZTE1R1NHAGcLmITAxdISnndtUm53X7p/FHatq0RJplOgPnAgOAXkB74PRot1fVu1W1QlUrSktL4w2jpRhTsl/TdrmVGlR1C/AvYGyYdRI/t/ftSCRMYxJqljkZWKOqlapaDTwBHAcUu800AH2AjQnGGDeruZtkEpH2ItIxMA2cCixrfqs4bV6akt2atiOR5P4ZMF5E2olTRZ4MLAfmAee760wHnkwsxPjFNBqkMS3rDrwhIu8D7wDPqOrzKSlpmzXHmMTE3c9dVReIyGPAYqAGeA+4G3gGeEREfuPOuzcZgcbDUrtJJlVdDYxIS2FDp8Lbd6alKONPCd3EpKrXA9eHzF5NmHZIL1ibu8lYk693kvvEH3kdiclQvh5+IMvXR2d8Ldu992/+zd7GYTKWr9OftbmbjGXnrkmQJXdjWiM7d02CfJfch9/wQv10+/xsDyMxxhjv+C657z7QMFhYj072NCVjTNvku+RujK906uN1BCZD+Xo8d2MyWp+xkNfe6yhMhrLkbkxrtX87bFridRQmQ1lyN6a12rbK6whMBrM2d2OM8SFL7sYY40OW3I0xxocsuRvT2n36itcRmAxkyd2Y1u6Za72OwGQgXyX3A9W1XodgTPJVfNPrCEwG8lVy37b3kNchGJN8BUVeR2AykK+Se+g4er88d6gncRiTVGID4JnY+Sq5Bw/xe90ZR9C72AYOMz6QZcndxM5XyT14COzidrneBWJMMlnN3cTBX8k9aLqmTj2Lw5iksgd3mDj4K7kH/RPU1FpyNz5hzTImDr5K7llBFRyruRvfOFjldQQmA/kquQfX3LPsm6xJERHJFpH3ROTptBT41BVpKcb4i7+Se9D0RWP7eRaH8b0rgRVeB2FMc/yV3IOye0GutVOa5BORPsBZwD0pL+z8+1JehPEvXyX3HfuqvQ7B+N8fgB8BdZFWEJGZIrJQRBZWVlbGX1K7kvi3NW2er5L7b5+xb8omdURkCrBFVRc1t56q3q2qFapaUVpaGn+BtTachomfr5J7bV3EypQxyXAccI6IrAUeASaJyN9TVtr2NSnbtfE/XyX3LLvZw6SQql6nqn1UtQy4EHhFVb+esgJtwDCTAF8ld7HkbvxEbQhrE7+EkruIFIvIYyLykYisEJFjRKSLiLwkIivd187JCrbleNJVkmnrVPVVVZ2S0kLqalK6e+NvidbcbweeV9UjgBE4fX9nAXNVdRAw132fFnbjkvGVWuv9ZeIXd3IXkSJgInAvgKoeUtWdwLnAbHe12cDURIOMlrW5G1+ps2YZE79Eau4DgErgfvdW7HtEpD3QXVU3uetsBrqH2zhpfYGDVNdabxnjI1m+uiRm0iyRsycHGA38RVVHAXsJaYJRVQXCjuCVtL7AQV5esSUp+zGmVRg1zesITAZLJLlvADao6gL3/WM4yf4LEekJ4L5axjUmHjl5XkdgMljcyV1VNwPrRWSwO2sysBx4CpjuzpsOPJlQhMYYY2KWk+D2VwBzRCQPWA1civOB8Q8RmQGsAy5IsAxjjDExSii5q+oSoCLMosmJ7DdR3Tvle1m8McZ4zpeX4+0hTMaYts43yf2C/3urfrrWsrvxm60rvY7AZBjfJPd31m6vn7b+7sZ3bPhfEyPfJPdgVQdsTA7jM9l2HcnExpfJ/edThngdgjHJZX3eTYx8mdwnHdHN6xCMSS6xZwKb2PgyuQ8oae91CMYY4ylfJndj/Md6gJnYWHI3JhPsXO91BCbDWHI3JhM8fZXXEZgMY8ndmExgF1RNjCy5G5MJsiy5m9hYcjcmE2Tneh2ByTCW3I3JBNl2E5OJjSV3YzJBVqKPXjBtjSV3Y6IkIgUi8o6IvC8iH4rIL9NWuCV3EyM7Y4yJ3kFgkqruEZFc4A0ReU5V3055yfkdUl6E8RdL7sZESVUV2OO+zXV/0nPraLehaSnG+Ic1yxgTAxHJFpElwBbgJVVdEGadmSKyUEQWVlZWJqfg4r7J2Y9pMyy5GxMDVa1V1ZFAH2CsiAwLs87dqlqhqhWlpaXpD9IYLLkbExdV3QnMA073OhZjwrHkbkyURKRURIrd6ULgFOAjb6MyJjxL7sZErycwT0SWAu/itLk/ndISv/liSndv/Mt6yxgTJVVdCoxKa6Gdeqa1OOMfVnM3xhgfsuRujDE+ZMndGGN8yHfJ/Y8XpbdJ1BhjWiPfJfe9B2u8DsEYYzznu+S+fd8hr0MwJvmevNzrCEyG8V1yr65JzzhOxqSF2vls4pNwcncHUnpPRJ523w8QkQUiskpEHhWRtD5Cprq2Lp3FGZNaddbMaOKTjJr7lcCKoPc3AbepajmwA5iRhDKidsiSuzHGJJbcRaQPcBZwj/tegEnAY+4qs4GpiZQRq1c/3pLO4oxJLWuWMXFKtOb+B+BHQKC63BXYqaqB75IbgN7hNkzJmNfA9ycPStq+jPGc2jdRE5+4k7uITAG2qOqieLZP1ZjXJxxu42cbP7Gau4lPIgOHHQecIyJnAgVAJ+B2oFhEctzaex9gY+JhGtNGWc3dxCnumruqXqeqfVS1DLgQeEVVv4bzAIPz3dWmA08mHGUMnGZ/Y3zC2txNnFLRz/3HwDUisgqnDf7eFJRhTNtgNXcTp6SM566qrwKvutOrgbHJ2K8xxmruJj6+u0PVGmWMr1jN3cTJd8ndGF+xNncTJ98l93Z52V6HYEzyWM3dxMl3yb261mo6xk/sfDbx8V1y319d63UIxiRPcG6vrfYsDJN5fJPcSzvmA9CpICkdgIxpHYKbZZY97l0cJuP4JhMO6taBsq7t7CYm4zPWLGPi45uauzGpJiJ9RWSeiCwXkQ9F5MqUFxpcc89O66MRTIbzTc3dmDSoAa5V1cUi0hFYJCIvqerylJUY3BUyOzdlxRj/sZq7MVFS1U2qutidrsJ5SE3YIa2TpkO3hulXb0ppUZ7asdYuGCeZL5L7gepa3vx0G6sr93odimkjRKQMGAUsCLMsec8q6HpYw/QXHyS2r9Zq7za4fQQ8+0OvI/EVXyT3O+auBGDb3kMeR2LaAhHpADwOXKWqu0OXp+pZBb51cJfzunqet3H4jC+S+75D1rfdpIeI5OIk9jmq+oTX8fiKDbWQVL5I7tb70aSD+4zge4EVqnqr1/H4h/0Dp4IvknuWZXeTHscBlwCTRGSJ+3Nmyksdcm7Ki2gdrOaeTL7oCmmp3aSDqr6BF6dbt6GwPK0PNEsvq5ylhD9q7ll2chgf0zZyTckq7knli+Ruqd34Wp3fk7v9B6eCL5K7nRvG19pKzd2q7knli+Qult2NnwXX3P3YXdDa3FPCF8ndmtyNrwUn95oD3sWRan784PKQT5K7ZXfjY75vlrH/31TwRXK33G587bDJXkeQJlZzTyafJHfL7sbHDj+1YXrTUu/iSBX7/00JfyR3rwMwJl1e+InXEaSOtbknlT+Su2V301ZsXOh1BClg/8Cp4IvkvmzjLq9DMMYkzGruyeSL5L7nYI3XIRjjT6rw7r1wsCp1ZdhX75TwRXI/aXC3llcyxsRu7evwzDXJeUrS0n/C1pWRl6eyzX3pP2Dly6nbfyvki+T+u+c+8joEY1qvujqYfQ6smhv7ttX7nde9WxOP44lvwZ/HhlmQhpr7E5fBnPNSX04rEndyF5G+IjJPRJaLyIcicqU7v4uIvCQiK93XzskL1xgTs0NVsOY1+Oc34tg4yYlX65pbmNyy2rhEau41wLWqOgQYD1wuIkOAWcBcVR0EzHXfG2O8llCzRwoTbzrb3N+6M31leSzu5K6qm1R1sTtdBawAenAgkWoAABBwSURBVAPnArPd1WYDUxMN0hiTiEDyjCNBBxJvtB8MX3wIvyqBXRtiLysd/dxfuC71ZbQSSWlzF5EyYBSwAOiuqpvcRZuB7hG2mSkiC0VkYWVlZTLCIDfbrrob00SsCbrxxrGtvvA+qKuGj59LXRkmKgkndxHpgPM0+KtUdXfwMlVVIlQXVPVuVa1Q1YrS0tJEwwDgnulHJ2U/xvhLMpJnjB8McX2QWJt7MiWU3EUkFyexz1HVJ9zZX4hIT3d5T2BLYiFGr6gwN11FGeOdJ74d54bxNMsENo122ziagILb3J+YCTcURb+tiSiR3jIC3AusUNVbgxY9BUx3p6cDaXuyr9rYFKYtWPpIbOsnpVkmym0TKUsVlj4a+3YmrJwEtj0OuAT4QESWuPN+AtwI/ENEZgDrgAsSCzF6W6oOpqsoYzJIEi6oprQsa3NPhbiTu6q+QeS/iicDUNfUWs3dpI6I3AdMAbao6jCv44laMroahquJ3zUROpfBBQ82LSvamvu6t+D5QG9p+/9NJl/coRrw0ebdLa9kTPweAE73Ooi4JbtZZtP7sDy01TXog2TtG077+YZmRrJ87FLYtCTy8mgtexz270h8Pz7iq+Q+pGcnr0MwPqaq84HtnhR+/n0JbJzOZpkAhVXuWC6rX22ugKBN4qy5b18Dj30THpsR3/bJtmcLrHnd6yj8ldxPHdrD6xCMSck9HOR1SHwfiXQ4iHbbRs0yUXyoJKPJKPDQ8HhunEqFe06G2VO8jsJfyT3LrsuYViAV93A0PyZLKrn/VGtfh1dvjH59NMrEHbxOvB8+CXwzSYWd67yOAPBZcrdnqRrfqqtNwk4SbJZZcFf06wfX9JsrNhn/s5LVtEyT+cm90ro/mrZgwMTE95Fw8ot1+2hq1Eloc6//QPHq200UNn8Atel9qFDGJ/cf/PN9r0MwbYSIPAy8BQwWkQ3uvRzpUZCMzgIJji0TU/LV6LpFJuXLditrlgm1dRX83wR4+fq0FpvxyX3/oWR8XTWmZap6kar2VNVcVe2jqvd6HVNMtA5evSm2bSTGNvFYL6gmo809oTtw02CvOwJLc11CUyDjk3tWxh+BMVHqf1zi+3j1f+LfNqrcGeaCarM192ReJ2ulyT1wTSA0vnVvOfcBVG1OSbEZnxpXbdnjdQjG+Fhws0wUbdqx1Nz3VCbnwdutteZeH0+EawIL/uK8rnszJcVnfHKvrWtlf1BjUiW4x8w+L+6liuZ/LYaukLeUw75tse2+pTJbk0Byj/jhk9q4Mz6552Zn/CEYEx0NSu7BX+UPVqWn1tpcGXeMgrtOCJ/QE41txdPNXyuoT54x7nfrqhgfKhKj9/7mTkRI4inuwpnxmdGSu2kz6oK60h3Y5bzuXA+/6wNv/wU+/BfUVjfdbsO78ZcZ7QXV7asbjxHTXLPMzvURdhJh/49+rYVrBUHlbPsUVs1tZt0gfxoDD18Y3brx2LHWeY2UxFPcnJTIkL+tQl6OJXfTRgQ3y3zyPPQ/puFuyMCzQSf+ECb9rPF2Ld0Kv6cScvIjdLeMtStkFBdU/zAMLn2++d1UH3CGFciO4gE8weX8cbQzfcOuKGJNsvm/h5pDDe8D37Tqf4URau7WLBNejo05YNqK0dMapiPd1BTP+Cq3lDckxYC6Oti6MmTFWLpCQrOd2Lcsbzov+EPg39+Fm/rDbdGMrJyEtuvFD8JfJ8W3bfUBp2nsld/A/Jsb5td/GAddUF32RNAHQGpvvsr45L7SesuYtmLIuQ3T9c0JIQk02kShCq/fCjs/c97vrWxc63zjVvhTReMkXHMgitp7uEQbZpuW4vz0Fed1fxQXjmNt3qje33TeU1fAxkXh1z+4Bz54LPL+/nS00zQWKnCMgfg2ve8Mcfzq7+KLO0YZn9yNaTOC29xrDzm169ALmNEm952fwdxfwkNBbc6/KW2oba5/x3kN/SbQ0hg3wb1fYn7+avB4NFFsU1cHHz9PzDX3PV9EGY/rmWvg8RmwcXH45bs+Cz8/8LfYu7Xx/N0bnVdrljHGAE5CD7Z3C3HX3AMJ5VBIP/NAGfUXAUP3p07if/ji8LtddL/zOu+3TWOLKc4oEt6798DDX4UP/ulukqIeQ7s/d14PxdhKEPggnHN+4/mR+r/v2w4fPetM1xx0PrwSYMndmEwR2hNGspuuE3Vyj9AkUN+UEKFWqXXwj+nw8TNRlhNmH8HlNJqnzS8Ptdv9VlG1KXI54ax8Kbr1AuIdmCzS+qHNNYHjfvgieOQiJ8n/phs8c3Vs5YXwTXL/76w4L4YYkyk69mz8/pby8M0yVV84t7XfUBR5X5G65wVqm5Hag7UuKPG3oLk25ZZq2dHUwkPjiLbm/uwPYNfGhvfv/T26cmJO7pGasEJubgq837HGea1xR7pd9EBs5YXIqOR+sKaWvQfDD5tpnWaM7+V3gNNbGPhL6+Dz91reV6TaaGitMlzNPTSpVkVqw26mLTxsooyx5h7a7BMYoAuc57c2J9AHHeDJy4PKDRNr3Mk9Us09QrNM4JtYknrPZFRy//KdbzL0+hfq3y9a1/BAXBuFwLQJtSHPL/g85OHSqlHWrJtJ3hC5Zq9hLuLee0r4IgL7ClujbukfNszy0Dbo5pLuA2e1sPsICXT5k073y0Db99ZVDb2IAsexpxL2bgu/fWgZj18Wuezgbzb3nQ5VnzcfW4wyKrl/+PnuRu8/2tzw/ovdB9IdjjHpVxDS1PL8jxu/1zp45+6W9xNIjIf2Nt0+eHmo3/Vp+hi5SI+Ve+XXzmu4bxKBro6Nym6ht8yLP3XLWw9LHoq/Rg2Na+vB/jkddq132r5rDjp3sa57o3E5t5TD7wc6TVg1h8LvJ3AMH/yj6fzl/4a7T2p8XeOzt4K2a4PJPVRhbsMFpeG9m2lfNMYvRk1rfrkqrGrhguH+HQ39xw82rjDVj7WSzHFP1r7edF645B4sXIILtI3PPtu5ySnwYOyVL8YeUzTPOQ3t9hn6/qGvOt1Ho90+2OeLiXgT0zPXNEzPPsfpZx+HjE7uWUFfD3NsjBnTFrT0AIM18xu/D62ZA9xUBn85Nvz27z/svCZSK45XTdDNRXVhxsgJdNMM9BsPJM+dEfqZ/+fKxOL57x8avw+98NrSh2igi2YkgQ+Y0A/QVS83TK95DT6NcqycEBmdEXsWFXgdgjHp17ks8rLqkGS+8L7Y9r3uv+4dnG7FqSbM3ZxeCdTUA5W6t//c/PqB3ibNNZ0057WQi9cxdf+kmd4yrsC3l2d/0Px6a8J884lCRif3XBs0zLRFeR2iX/fFn7W8Tqjf9mhoK06wO169bZ8mZz8v/rxpU1JzDuxuvukkVml+yDUA7/41rs0yZlTIS+5dUD9dNqvxJ+hfp1WkOxxjvFPnQYJJVOjAZPF6847Y1r+xb3LKDfh11+TuL4Uypur7+sqtEZfd+8bqNEZijMfCjdluTIiMSe7NOeuoni2vZIxfHHm283r46d7GYVo1XyT3KcN7eR2CMekz+RfwozVQfrLXkZhWLCXJXUROF5GPRWSViMxKZF9ls55p0sYeKjvbxh4w6ZHMcztuWdnQrkvTUSKNP513b1ybJT25i0g28GfgDGAIcJGIDIlnX3+etyqq9bJbesq6MUmQzHM7KQ6b3Ph9yeD0x1DcP/1lhjMr6LmsY7+dvnK7DkrNfnPbN0z3ia/DSCp6y4wFVqnqagAReQQ4FwjzXK3m/f6Fj6NaL9tGDTPpkbRzOym6HQGdB8DRM+DYK+DDfzu3z1/7iTOI1vxbYOxl0O8Y5zFyT1/VsO2knzsPtV4yx0nQ5/zR6eMe2rcbIDvfGdPmsnnON4atK2HzUug9BrqWw21DW451wtWQ3xEWPtDwcIuy4xvuXr1qmfNsVYCZrznPc100u+mNRF0Og+5DYMV/nPeDToWzbnXWD35u6nHfh9duhuw86DEMhp0Pee2dm7z6joOljzS+yelLdztJtOYg/OUYZ97F/4D8Ts4DS574ljPvK7OdB5IU94fl/4Iptztj2D/7AzhsUkPf9cvmOaN43noEDP0yDL/AeXrWZfOg+7Dmu2cOvxAm/RT+cBSceF3z9zU0QzTJA9yLyPnA6ar6Lff9JcA4Vf1eyHozgZkA/fr1G7NuXdPbgb/zt0U8/+HmZssrKszl/etPTVL0xo9EZJGqJtxfNpnntmc2L3OSXJcBzvt9252EHSwwQNf6BZDXDnoMdx5YUdS7+X1v+xS6Htbwfvsa54aorZ/A0KnOvP07nDtMSwbBoX3w+i1wwo+dB3SHOrjH+bA5/lrnwRzdjoQjWhgQLBYHdsGGhVA2oXH51fudO3SD56170xk3fth5TfejCm/+EUZe7Pxuaw5CYbGzrPITKO4HuQXOs1Zzw9x4qQoHdjofonntYjqE5s5tz5J7sIqKCl24cGFS4zAmIN3JPZid2yaVmju3U3FBdSMQfOdAH3eeMZnOzm2TMVKR3N8FBonIABHJAy4EnkpBOcakm53bJmMk/YKqqtaIyPeAF4Bs4D5V/TDZ5RiTbnZum0ySkrFlVPVZ4NlU7NsYL9m5bTKFL+5QNcYY05gld2OM8SFL7sYY40OW3I0xxoeSfhNTXEGIVAKRbuMrASIP5p457Di8019Vk/g4nujZuZ1RMvE4Ip7brSK5N0dEFibj7kKv2XGYUH75XdpxtE7WLGOMMT5kyd0YY3woE5L73V4HkCR2HCaUX36XdhytUKtvczfGGBO7TKi5G2OMiZEld2OM8aFWm9xbxYOIwxCR+0Rki4gsC5rXRUReEpGV7mtnd76IyB3uMSwVkdFB20x3118pItOD5o8RkQ/cbe4QSf4DYkWkr4jME5HlIvKhiFyZiceRqVrjue2H89otx87tAFVtdT84w6l+CgwE8oD3gSFex+XGNhEYDSwLmnczMMudngXc5E6fCTwHCDAeWODO7wKsdl87u9Od3WXvuOuKu+0ZKTiGnsBod7oj8AnOA58z6jgy8ae1ntt+OK/dcuzcdn9aa829/kHEqnoICDyI2HOqOh/YHjL7XGC2Oz0bmBo0/0F1vA0Ui0hP4DTgJVXdrqo7gJeA091lnVT1bXXOogeD9pXMY9ikqovd6SpgBdA7044jQ7XKc9sP57V7HHZuu1prcu8NrA96v8Gd11p1V9VN7vRmoLs7Hek4mpu/Icz8lBGRMmAUsIAMPo4MkknndkafD2393G6tyT1juZ/mGdG/VEQ6AI8DV6nq7uBlmXQcJvUy7Xywc7v1JvdMexDxF+7XNdzXLe78SMfR3Pw+YeYnnYjk4pz8c1T1CXd2xh1HBsqkczsjzwc7tx2tNbln2oOInwICV9OnA08GzZ/mXpEfD+xyvxq+AJwqIp3dq/anAi+4y3aLyHj3Cvy0oH0ljbvve4EVqnprph5Hhsqkczvjzgc7t4N4fUU30g/OVexPcHoW/NTreILiehjYBFTjtLfNALoCc4GVwMtAF3ddAf7sHsMHQEXQfr4JrHJ/Lg2aXwEsc7f5E+5dxEk+hgk4X0uXAkvcnzMz7Tgy9ac1ntt+OK/dcuzcdn9s+AFjjPGh1tosY4wxJgGW3I0xxocsuRtjjA9ZcjfGGB+y5G6MMT5kyd0YY3zIkrsxxvjQ/wMDupKJKWso5gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1naJFzz_eH5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!zip -r saveFolder3.zip saveFolder2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaRGE5obeTdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"saveFolder.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78zvX1zFQS1V",
        "colab_type": "text"
      },
      "source": [
        "START EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhn91G1uQY_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_list(list_path):\n",
        "    img_list = []\n",
        "    with open(list_path, 'r') as f:\n",
        "        for line in f.readlines()[0:]:\n",
        "            img_path = line.strip().split()\n",
        "            img_list.append(img_path[0])\n",
        "    print('There are {} images..'.format(len(img_list)))\n",
        "    return img_list\n",
        "\n",
        "def save_feature(save_path, img_name, features):\n",
        "    img_path = os.path.join(save_path, img_name)\n",
        "    img_dir  = os.path.dirname(img_path) + '/';\n",
        "    if not os.path.exists(img_dir):\n",
        "        os.makedirs(img_dir)\n",
        "    fname = os.path.splitext(img_path)[0]\n",
        "    fname = fname + '.feat'\n",
        "    fid   = open(fname, 'wb')\n",
        "    fid.write(features)\n",
        "    fid.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCtpzeDjQi5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LightCNN_29Layers(num_classes=501)\n",
        "model.eval()\n",
        "model = torch.nn.DataParallel(model).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKA1_3chQlG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if os.path.isfile(\"saveFolder2/lightCNN_501_checkpoint.pth.tar\"):\n",
        "  print(\"=> loading checkpoint '{}'\".format(\"saveFolder2/lightCNN_501_checkpoint.pth.tar\"))\n",
        "  checkpoint = torch.load(\"saveFolder2/lightCNN_501_checkpoint.pth.tar\")\n",
        "  model.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96p-7G-yfEGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_list  = read_list(\"test.txt\")\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "count     = 0\n",
        "input     = torch.zeros(1, 1, 128, 128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYDeRf60fHlM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for img_name in img_list:\n",
        "  count = count + 1\n",
        "  img   = cv2.imread(os.path.join(\"CASIA3/\", img_name), cv2.IMREAD_GRAYSCALE)\n",
        "  #img = cv2.resize(img, (128, 128), interpolation = cv2.INTER_NEAREST )\n",
        "  #img   = np.reshape(img, (128, 128, 1))\n",
        "  img   = transform(img)\n",
        "  input[0,:,:,:] = img\n",
        "        \n",
        "  start = time.time()\n",
        "  input = input.cuda()\n",
        "  input_var   = torch.autograd.Variable(input, volatile=True)\n",
        "  _, features = model(input_var)\n",
        "  end         = time.time() - start\n",
        "  print(\"{}({}/{}). Time: {}\".format(os.path.join(\"CASIA-WebFace/\", img_name), count, len(img_list), end))\n",
        "  save_feature(\"save2/\", img_name, features.data.cpu().numpy()[0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnCNXi6H7mDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!zip -r saveFeatures500.zip save2"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}